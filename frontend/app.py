from datetime import datetime
from pytz import timezone
import os
import json
import time
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
import requests
import re
import traceback
from typing import List, Dict, Any, Optional, Union, Tuple
from dotenv import load_dotenv
import base64
import io
import zipfile
from pathlib import Path
import streamlit as st
import streamlit.components.v1 as components
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import matplotlib.font_manager as fm
import tempfile
import shutil
import matplotlib.pyplot as plt
from openai import OpenAI

def jst_now_str():
    return datetime.now(timezone('Asia/Tokyo')).strftime('%Y-%m-%d %H:%M:%S JST')

# ---# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š
def get_japanese_font():
    """åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’æ¤œå‡ºã—ã¦è¿”ã™"""
    try:
        # ä¸€èˆ¬çš„ãªæ—¥æœ¬èªå¯¾å¿œãƒ•ã‚©ãƒ³ãƒˆã®å„ªå…ˆé †ä½
        font_preferences = [
            'IPAexGothic', 'IPAGothic', 'Noto Sans CJK JP', 'Noto Sans JP',
            'Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'Meiryo', 'MS Gothic',
            'Yu Gothic', 'TakaoGothic', 'VL Gothic', 'Arial Unicode MS', 'sans-serif'
        ]
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ³ãƒˆã‚’å–å¾—
        import matplotlib.font_manager as fm
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        
        # åˆ©ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ³ãƒˆã‹ã‚‰å„ªå…ˆé †ã«é¸æŠ
        for font in font_preferences:
            if any(font.lower() in f.lower() for f in available_fonts):
                return font
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚µãƒ³ã‚»ãƒªãƒ•ãƒ•ã‚©ãƒ³ãƒˆ
        return 'sans-serif'
    except Exception as e:
        print(f"ãƒ•ã‚©ãƒ³ãƒˆæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return 'sans-serif'

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š
japanese_font = get_japanese_font()

def plot_overlap_comparison(results_df: pd.DataFrame) -> None:
    """
    ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã”ã¨ã®è©•ä¾¡æŒ‡æ¨™ã‚’æ¯”è¼ƒã™ã‚‹ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã™ã‚‹
    
    Args:
        results_df: è©•ä¾¡çµæœã®DataFrame
    """
    try:
        # å¿…è¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å®šç¾©
        required_columns = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness', 'overall_score']
        available_metrics = [col for col in required_columns if col in results_df.columns]
        
        if not available_metrics:
            st.warning("æ¯”è¼ƒå¯èƒ½ãªè©•ä¾¡æŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æƒ…å ±ã‚’æº–å‚™
        if 'overlap' not in results_df.columns and 'chunk_overlap' in results_df.columns:
            results_df['overlap'] = results_df['chunk_overlap']
        elif 'overlap' not in results_df.columns and 'contexts' in results_df.columns:
            try:
                results_df['overlap'] = results_df['contexts'].apply(
                    lambda x: len(' '.join(x).split()) - len(set(' '.join(x).split())) if x and len(x) > 0 else 0
                )
            except Exception as e:
                st.warning(f"ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æƒ…å ±ã®è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                return
        
        if 'overlap' not in results_df.columns:
            st.warning("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ¯”è¼ƒã«ã¯'overlap'åˆ—ã¾ãŸã¯'chunk_overlap'åˆ—ãŒå¿…è¦ã§ã™ã€‚")
            return
        
        # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã«ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ ã‚’æ±ºå®š
        group_cols = ['overlap']
        if 'embedding_model' in results_df.columns:
            group_cols.append('embedding_model')
        if 'chunk_strategy' in results_df.columns:
            group_cols.append('chunk_strategy')
        if 'chunk_size' in results_df.columns:
            group_cols.append('chunk_size')
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ
        overlap_scores = results_df.groupby(group_cols)[available_metrics].mean().reset_index()
        
        if len(overlap_scores) <= 1:
            st.warning(f"ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å€¤ãŒ1ç¨®é¡ã—ã‹ã‚ã‚Šã¾ã›ã‚“ï¼ˆå€¤: {results_df['overlap'].iloc[0]}ï¼‰ã€‚æ¯”è¼ƒã«ã¯è¤‡æ•°ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—å€¤ãŒå¿…è¦ã§ã™ã€‚")
            return
        
        # ã‚¿ãƒ–ã§è¤‡æ•°ã®å¯è¦–åŒ–ã‚’è¡¨ç¤º
        tab1, tab2, tab3 = st.tabs(["æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•", "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "æœ€é©å€¤ã‚µãƒãƒªãƒ¼"])
        
        with tab1:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã”ã¨ã«å€‹åˆ¥ã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
            # --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨: å…¨ã‚°ãƒ©ãƒ•ãƒ»ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¸€æ™‚ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„ ---
            all_figs = []
            all_tables = []

            for metric in available_metrics:
                st.subheader(f"{metric} ã®æ¯”è¼ƒ")
                
                # ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®ä¸¡æ–¹ã®æƒ…å ±ãŒã‚ã‚‹å ´åˆ
                if 'embedding_model' in group_cols and 'chunk_size' in group_cols:
                    models = overlap_scores['embedding_model'].unique()
                    chunk_sizes = sorted(overlap_scores['chunk_size'].unique())
                    
                    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®ã‚¿ãƒ–ã‚’ä½œæˆ
                    model_tabs = st.tabs([f"{model}" for model in models])
                    
                    for tab_idx, model in enumerate(models):
                        with model_tabs[tab_idx]:
                            model_data = overlap_scores[overlap_scores['embedding_model'] == model]
                            
                            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã”ã¨ã«ç•°ãªã‚‹è‰²ã‚’å‰²ã‚Šå½“ã¦
                            colors = px.colors.qualitative.Plotly
                            
                            # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
                            fig = go.Figure()
                            
                            # å„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                            for i, chunk_size in enumerate(chunk_sizes):
                                # åŒã˜ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºå†…ã§é‡è¤‡ã™ã‚‹ overlap ã”ã¨ã®å€¤ã‚’å¹³å‡ã§é›†ç´„
                                size_data_raw = model_data[model_data['chunk_size'] == chunk_size]
                                if len(size_data_raw) == 0:
                                    continue

                                # é‡è¤‡ã—ã¦ã„ã‚‹ (overlap) ã‚’ã¾ã¨ã‚ã¦å¹³å‡å€¤ã‚’ç®—å‡º
                                size_data = (
                                    size_data_raw
                                    .groupby('overlap', as_index=False)[metric]
                                    .mean()
                                )

                                color_idx = i % len(colors)

                                # ãƒ©ãƒ™ãƒ«åˆ—ã‚’ä½¿ç”¨ã—ã¦æˆ¦ç•¥åã‚’å–å¾—ï¼ˆé‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã‚‚1ã¤ã«ã¾ã¨ã‚ã‚‹ï¼‰
                                if 'label' in size_data_raw.columns:
                                    display_strategy = size_data_raw['label'].iloc[0]
                                else:
                                    # ãƒ©ãƒ™ãƒ«åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
                                    strategy = size_data_raw['chunk_strategy'].iloc[0]
                                    if isinstance(strategy, str):
                                        base_strategy = strategy.split('-')[0].lower()
                                        if base_strategy in ['semantic', 'sentence', 'paragraph']:
                                            display_strategy = base_strategy
                                        else:
                                            display_strategy = f"{base_strategy}-{chunk_size}"
                                    else:
                                        display_strategy = str(strategy)

                                # ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ±ºå®š
                                if isinstance(display_strategy, str) and any(s in display_strategy for s in ['semantic', 'sentence', 'paragraph']):
                                    hover_text = f'<b>{display_strategy}</b><br>ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: %{{x}}<br>ã‚¹ã‚³ã‚¢: %{{y:.3f}}<extra></extra>'
                                else:
                                    hover_text = f'<b>{display_strategy} (ãƒãƒ£ãƒ³ã‚¯: {chunk_size})</b><br>ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: %{{x}}<br>ã‚¹ã‚³ã‚¢: %{{y:.3f}}<extra></extra>'

                                # é›†ç´„å¾Œã®ãƒ‡ãƒ¼ã‚¿ã§æç”»
                                fig.add_trace(go.Scatter(
                                    x=size_data['overlap'],
                                    y=size_data[metric],
                                    name=display_strategy,
                                    mode='lines+markers',
                                    line=dict(width=3, color=colors[color_idx]),
                                    marker=dict(size=10, color=colors[color_idx]),
                                    hovertemplate=hover_text,
                                    showlegend=True
                                ))
                            
                            # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                            # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åã‚’å–å¾—ï¼ˆcreate_labelé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
                            strategy_name = 'ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ¥æ¯”è¼ƒ'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                            if 'chunk_strategy' in model_data.columns:
                                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€åˆã®è¡Œã‹ã‚‰æˆ¦ç•¥åã‚’å–å¾—
                                strategy = model_data.iloc[0]
                                # create_labelé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦æˆ¦ç•¥åã‚’ç”Ÿæˆ
                                base_strategy = create_label(strategy)
                                if base_strategy in ['semantic', 'sentence', 'paragraph']:
                                    strategy_name = f"{base_strategy}æˆ¦ç•¥"
                                else:
                                    strategy_name = f"{base_strategy}æˆ¦ç•¥ - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ¥æ¯”è¼ƒ"
                            
                            fig.update_layout(
                                title=f"{model} - {strategy_name}",
                                xaxis_title="ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)",
                                yaxis_title=f"{metric} ã‚¹ã‚³ã‚¢ (0-1)",
                                template='plotly_white',
                                height=400,
                                margin=dict(l=50, r=50, t=80, b=50),
                                legend=dict(
                                    orientation='h',
                                    yanchor='bottom',
                                    y=1.02,
                                    xanchor='right',
                                    x=1,
                                    bgcolor='rgba(255,255,255,0.9)',
                                    bordercolor='rgba(0,0,0,0.2)',
                                    borderwidth=1
                                ),
                                xaxis=dict(
                                    showgrid=True,
                                    gridwidth=1,
                                    gridcolor='rgba(0,0,0,0.1)'
                                ),
                                yaxis=dict(
                                    range=[0, 1.05],
                                    showgrid=True,
                                    gridwidth=1,
                                    gridcolor='rgba(0,0,0,0.1)'
                                )
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã«figã‚’ä¿å­˜
                            all_figs.append((f"{metric}_{model}_chunk.png", fig))
                
                # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ã¿ã‚ã‚‹å ´åˆ
                elif 'embedding_model' in group_cols:
                    # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹è‰²ã‚’å‰²ã‚Šå½“ã¦
                    colors = px.colors.qualitative.Plotly
                    
                    # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
                    fig = go.Figure()
                    
                    # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    for i, model in enumerate(overlap_scores['embedding_model'].unique()):
                        model_data = overlap_scores[overlap_scores['embedding_model'] == model]
                        color_idx = i % len(colors)
                        
                        fig.add_trace(go.Scatter(
                            x=model_data['overlap'],
                            y=model_data[metric],
                            name=model,
                            mode='lines+markers',
                            line=dict(width=3, color=colors[color_idx]),
                            marker=dict(size=10, color=colors[color_idx]),
                            hovertemplate=f'<b>{model}</b><br>ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: %{{x}}<br>ã‚¹ã‚³ã‚¢: %{{y:.3f}}<extra></extra>'
                        ))
                    
                    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
                    fig.update_layout(
                        xaxis_title="ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)",
                        yaxis_title=f"{metric} ã‚¹ã‚³ã‚¢ (0-1)",
                        template='plotly_white',
                        height=400,
                        margin=dict(l=50, r=50, t=50, b=50),
                        legend=dict(
                            orientation='h',
                            yanchor='bottom',
                            y=1.02,
                            xanchor='right',
                            x=1,
                            bgcolor='rgba(255,255,255,0.9)',
                            bordercolor='rgba(0,0,0,0.2)',
                            borderwidth=1
                        ),
                        xaxis=dict(
                            showgrid=True,
                            gridwidth=1,
                            gridcolor='rgba(0,0,0,0.1)'
                        ),
                        yaxis=dict(
                            range=[0, 1.05],
                            showgrid=True,
                            gridwidth=1,
                            gridcolor='rgba(0,0,0,0.1)',
                            showline=True,
                            linewidth=2,
                            linecolor='black',
                            mirror=True,
                            ticks='outside',
                            tickwidth=2,
                            tickcolor='black',
                            ticklen=6
                        )
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãŒãªã„å ´åˆ
                else:
                    # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚°ãƒ©ãƒ•
                    fig = px.line(
                        overlap_scores, 
                        x='overlap', 
                        y=metric,
                        title=f"{metric} ã‚¹ã‚³ã‚¢",
                        labels={'overlap': 'ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)', metric: 'ã‚¹ã‚³ã‚¢ (0-1)'},
                        markers=True
                    )
                    
                    fig.update_traces(
                        line=dict(width=3),
                        marker=dict(size=10)
                    )
                    
                    fig.update_layout(
                        height=400,
                        showlegend=False,
                        xaxis=dict(showgrid=True, gridwidth=1, gridcolor='rgba(0,0,0,0.1)'),
                        yaxis=dict(range=[0, 1.05], showgrid=True, gridwidth=1, gridcolor='rgba(0,0,0,0.1)'),
                        margin=dict(l=50, r=50, t=50, b=50)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é–“ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
                st.markdown("<br>", unsafe_allow_html=True)

        with tab2:
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è¡¨ç¤º
            if 'chunk_size' in group_cols and 'embedding_model' in group_cols:
                # ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã”ã¨ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                for model in overlap_scores['embedding_model'].unique():
                    model_data = overlap_scores[overlap_scores['embedding_model'] == model]
                    
                    # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
                    pivot_data = model_data.pivot_table(
                        index='chunk_size',
                        columns='overlap',
                        values='overall_score',
                        aggfunc='mean'
                    ).sort_index(ascending=False)
                    
                    if not pivot_data.empty:
                        fig = px.imshow(
                            pivot_data,
                            labels=dict(
                                x="ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)", 
                                y="ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)", 
                                color="ã‚¹ã‚³ã‚¢ (0-1)"
                            ),
                            title=f"{model} - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®é–¢ä¿‚",
                            color_continuous_scale='Viridis',
                            aspect="auto"
                        )
                        fig.update_layout(
                            xaxis_title="ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)",
                            yaxis_title="ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º (ãƒˆãƒ¼ã‚¯ãƒ³æ•°)",
                            coloraxis_colorbar_title="ã‚¹ã‚³ã‚¢ (0-1)"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒªã‚¹ãƒˆã«å¿…ãšè¿½åŠ 
                        all_figs.append((f"heatmap_{model}.png", fig))
        
        with tab3:
            # æœ€é©ãªã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã®ã‚µãƒãƒªãƒ¼
            if 'embedding_model' in group_cols and 'chunk_strategy' in group_cols and 'chunk_size' in group_cols:
                # æœ€é©ãªã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚ºã‚’è¦‹ã¤ã‘ã‚‹
                best_overlaps = []
                
                for model in overlap_scores['embedding_model'].unique():
                    model_data = overlap_scores[overlap_scores['embedding_model'] == model]
                    for strategy in model_data['chunk_strategy'].unique():
                        strategy_data = model_data[model_data['chunk_strategy'] == strategy]
                        for size in strategy_data['chunk_size'].unique():
                            size_data = strategy_data[strategy_data['chunk_size'] == size]
                            if not size_data.empty:
                                best_idx = size_data['overall_score'].idxmax()
                                best_overlaps.append({
                                    'ãƒ¢ãƒ‡ãƒ«': model,
                                    'ãƒãƒ£ãƒ³ã‚¯åŒ–æ–¹æ³•': strategy,
                                    'ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º': size,
                                    'æœ€é©ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—': size_data.loc[best_idx, 'overlap'],
                                    'æœ€é«˜ã‚¹ã‚³ã‚¢': round(size_data.loc[best_idx, 'overall_score'], 3)
                                })
                
                if best_overlaps:
                    summary_df = pd.DataFrame(best_overlaps)
                    st.dataframe(
                        summary_df.sort_values(['ãƒ¢ãƒ‡ãƒ«', 'ãƒãƒ£ãƒ³ã‚¯åŒ–æ–¹æ³•', 'ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º']),
                        column_config={
                            'æœ€é«˜ã‚¹ã‚³ã‚¢': st.column_config.ProgressColumn(
                                'æœ€é«˜ã‚¹ã‚³ã‚¢',
                                format='%.3f',
                                min_value=0,
                                max_value=1.0
                            )
                        },
                        use_container_width=True
                    )
                    # ã‚µãƒãƒªãƒ¼è¡¨ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    all_tables.append(("summary.csv", summary_df))
                else:
                    st.info("ğŸ“„ PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        
        # è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆé›†è¨ˆæ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ãƒªã‚¹ãƒˆã«è¿½åŠ 
        all_tables.append(("detail.csv", overlap_scores))

        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
            st.dataframe(overlap_scores.style.background_gradient(
                subset=available_metrics, cmap='YlGnBu'
            ), use_container_width=True)

        # --- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’é–¢æ•°æœ«å°¾ã§è¡¨ç¤º ---
        import io, zipfile
        from datetime import datetime
        import plotly.io as pio
        if st.button("å…¨ã‚°ãƒ©ãƒ•ãƒ»è¡¨ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (zip)"):
            # é€²æ—ãƒãƒ¼ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚’ç”¨æ„
            progress_bar = st.progress(0)
            total_tasks = len(all_figs) + len(all_tables)
            current = 0
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                # ã‚°ãƒ©ãƒ•ç”»åƒ
                for fname, fig in all_figs:
                    img_bytes = fig.to_image(format="png")
                    zf.writestr(fname, img_bytes)
                    current += 1
                    progress_bar.progress(current / total_tasks)
                # ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆcsvï¼‰
                for tname, df in all_tables:
                    zf.writestr(tname, df.to_csv(index=False, encoding='utf-8'))
                    current += 1
                    progress_bar.progress(current / total_tasks)
            zip_buffer.seek(0)
            progress_bar.empty()  # ãƒãƒ¼ã‚’æ¶ˆã™
            st.download_button(
                label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹",
                data=zip_buffer,
                file_name=f"overlap_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                mime="application/zip"
            )
    
    except Exception as e:
        st.error(f"ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ¯”è¼ƒã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.error(f"ã‚¨ãƒ©ãƒ¼ã®è©³ç´°: {traceback.format_exc()}")

# --- ã‚°ãƒ©ãƒ•ä¿å­˜ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def save_plotly_figure(fig, filename: str, width: int = 1200, height: int = 800, scale: float = 3.0) -> bytes:
    """
    Plotlyã®å›³ã‚’ç”»åƒãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ã™ã‚‹
    
    Args:
        fig: Plotlyã®å›³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        filename: ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ã¯ä¸è¦ï¼‰
        width: ç”»åƒã®å¹…ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
        height: ç”»åƒã®é«˜ã•ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
        scale: ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆè§£åƒåº¦ã‚’ä¸Šã’ã‚‹å ´åˆï¼‰
        
    Returns:
        bytes: ç”»åƒãƒ‡ãƒ¼ã‚¿ï¼ˆPNGå½¢å¼ï¼‰
    """
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦è¨­å®š
    fig.update_layout(
        font_family=japanese_font,
        title_font_family=japanese_font,
        font=dict(family=f"{japanese_font}, Arial, sans-serif")
    )
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆæ—¥æœ¬èªæ–‡å­—åŒ–ã‘å¯¾ç­–ï¼‰
    temp_dir = tempfile.mkdtemp()
    try:
        temp_file = os.path.join(temp_dir, f"{filename}.png")
        fig.write_image(temp_file, width=width, height=height, scale=scale)
        with open(temp_file, 'rb') as f:
            img_data = f.read()
        return img_data
    except Exception as e:
        st.error(f"ç”»åƒã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None
    finally:
        shutil.rmtree(temp_dir, ignore_errors=True)

def create_zip_with_graphs(bulk_results: Union[dict, list], filename: str = "graphs") -> Optional[bytes]:
    """
    ä¸€æ‹¬è©•ä¾¡çµæœã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦è¿”ã™
    
    Args:
        bulk_results: ä¸€æ‹¬è©•ä¾¡çµæœï¼ˆè¾æ›¸ã¾ãŸã¯ãƒªã‚¹ãƒˆï¼‰
        filename: ç”Ÿæˆã™ã‚‹ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹åï¼ˆæ‹¡å¼µå­ã¯ä¸è¦ï¼‰
        
    Returns:
        Optional[bytes]: ç”Ÿæˆã•ã‚ŒãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã€‚ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã‚’è¿”ã™
    """
    # é€²æ—è¡¨ç¤ºç”¨ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ä½œæˆ
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # é€²æ—çŠ¶æ³ã‚’æ›´æ–°ã™ã‚‹é–¢æ•°
    def update_progress(current: int, total: int, message: str) -> None:
        """é€²æ—çŠ¶æ³ã‚’æ›´æ–°ã™ã‚‹"""
        progress = int((current / total) * 100) if total > 0 else 0
        progress_bar.progress(progress)
        status_text.text(f"é€²æ—: {current}/{total} - {message}")

    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    temp_dir = tempfile.mkdtemp()
    saved_files = []
    
    try:
        # çµæœã‚’DataFrameã«å¤‰æ›
        if isinstance(bulk_results, list):
            results_df = pd.DataFrame(bulk_results)
        else:
            results_df = pd.DataFrame([bulk_results])
            
        # å‡¦ç†ã™ã‚‹ã‚°ãƒ©ãƒ•ã®ç·æ•°ã‚’è¨ˆç®—
        total_graphs = 0
        if not results_df.empty:
            # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã¨ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®æ•°
            total_graphs += len(results_df['embedding_model'].unique()) * 2
            # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®æ•°
            if 'chunk_strategy' in results_df.columns:
                total_graphs += len(results_df['chunk_strategy'].unique())
                
        if total_graphs == 0:
            status_text.warning("ç”Ÿæˆã™ã‚‹ã‚°ãƒ©ãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
            
        current_graph = 0
    
        # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã€ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
        required_cols = {
            'avg_chunk_len', 'num_chunks', 'overall_score', 'chunk_strategy', 'embedding_model',
            'faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness'
        }
        
        # ä¸è¶³ã‚«ãƒ©ãƒ ã®è£œå®Œ
        for col in required_cols:
            if col not in results_df.columns:
                if col == 'chunk_strategy':
                    results_df[col] = 'unknown'
                else:
                    results_df[col] = 0.5
    
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’å®šç¾©
        metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision", "answer_correctness"]
        metrics_jp = ["ä¿¡é ¼æ€§", "å›ç­”ã®é–¢é€£æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§", "å›ç­”ã®æ­£ç¢ºæ€§"]
        
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        if 'embedding_model' in results_df.columns:
            model_groups = list(results_df.groupby('embedding_model'))
        else:
            model_groups = [('default', results_df)]
    
        # å„ã‚°ãƒ©ãƒ•ã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
        
        # 1. ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜
        for model_name, model_data in model_groups:
            if not model_data.empty and 'chunk_size' in model_data.columns and 'overall_score' in model_data.columns:
                # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
                fig_bubble = px.scatter(
                    model_data,
                    x="num_chunks",
                    y="avg_chunk_len",
                    size=[min(s * 8, 20) for s in model_data["overall_score"]],  # ãƒãƒ–ãƒ«ã®ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ãèª¿æ•´
                    color="overall_score",
                    hover_name=model_data['chunk_strategy'] + '-' + model_data['chunk_size'].astype(str),
                    text=model_data['chunk_strategy'],
                    title=f"{model_name} - ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                    labels={
                        "num_chunks": "ãƒãƒ£ãƒ³ã‚¯æ•°",
                        "avg_chunk_len": "å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º (æ–‡å­—æ•°)",
                        "overall_score": "ç·åˆã‚¹ã‚³ã‚¢"
                    },
                    color_continuous_scale=px.colors.sequential.Viridis,
                    color_continuous_midpoint=0.5,
                )
                
                # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ›´æ–°
                fig_bubble.update_traces(
                    # ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ¥é€”æ³¨é‡ˆã¨ã—ã¦é…ç½®ã™ã‚‹ãŸã‚ã€ãƒãƒ–ãƒ«å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã¯éè¡¨ç¤ºã«
                    texttemplate='',  # ç©ºã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ãƒ†ã‚­ã‚¹ãƒˆã‚’éè¡¨ç¤ºã«
                    marker=dict(
                        line=dict(width=1.5, color='rgba(0,0,0,0.7)'),
                        opacity=0.8,
                        sizemode='diameter',
                        sizemin=6,  # æœ€å°ã‚µã‚¤ã‚ºã‚’ã•ã‚‰ã«å°ã•ã
                        sizeref=0.1  # ã‚µã‚¤ã‚ºã®æ„Ÿåº¦ã‚’èª¿æ•´
                    ),
                    hovertemplate=
                    '<b>%{hovertext}</b><br><br>' +
                    'ãƒãƒ£ãƒ³ã‚¯æ•°: <b>%{x}</b><br>' +
                    'å¹³å‡ã‚µã‚¤ã‚º: <b>%{y}æ–‡å­—</b><br>' +
                    'ã‚¹ã‚³ã‚¢: <b>%{marker.color:.2f}</b><extra></extra>',
                    hoverlabel=dict(
                        font_size=14,
                        font_family=japanese_font,
                        bgcolor='white',
                        bordercolor='#333',
                        font_color='#333'
                    )
                )
                
                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                fig_bubble.update_layout(
                    title={
                        'text': f"{model_name} - ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                        'x': 0.5,
                        'xanchor': 'center',
                        'font': {
                            'size': 20,
                            'family': japanese_font,
                            'color': '#333'
                        },
                        'y': 0.95,
                        'yanchor': 'top'
                    },
                    coloraxis_colorbar=dict(
                        title=dict(
                            text='ã‚¹ã‚³ã‚¢',
                            font=dict(
                                size=14,
                                family=japanese_font
                            )
                        ),
                        tickfont=dict(
                            family=japanese_font,
                            size=12
                        )
                    ),
                    font=dict(
                        size=14,
                        family=japanese_font,
                        color='#333'
                    ),
                    height=600,
                    margin=dict(l=80, r=50, t=100, b=120),  # ä¸‹éƒ¨ã®ä½™è£•ã‚’å¢—ã‚„ã—ã¦æ³¨é‡ˆç”¨ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’ç¢ºä¿
                    plot_bgcolor='white',
                    paper_bgcolor='white',
                    xaxis=dict(
                        title=dict(
                            text='ãƒãƒ£ãƒ³ã‚¯æ•°',
                            font=dict(
                                size=14,
                                family=japanese_font
                            )
                        ),
                        tickfont=dict(
                            size=12,
                            family=japanese_font
                        ),
                        gridcolor='rgba(0,0,0,0.1)',
                        showline=True,
                        linewidth=1,
                        linecolor='#ddd',
                        mirror=True
                    ),
                    yaxis=dict(
                        title=dict(
                            text='å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º (æ–‡å­—æ•°)',
                            font=dict(
                                size=14,
                                family=japanese_font
                            )
                        ),
                        tickfont=dict(
                            size=12,
                            family=japanese_font
                        ),
                        gridcolor='rgba(0,0,0,0.1)',
                        showline=True,
                        linewidth=1,
                        linecolor='#ddd',
                        mirror=True
                    )
                )
                
                # ãƒãƒ–ãƒ«ã«æ³¨é‡ˆã‚’è¿½åŠ ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ–ãƒ«ã®å¤–å´ã«é…ç½®ï¼‰
                for i, row in model_data.iterrows():
                    fig_bubble.add_annotation(
                        x=row['num_chunks'],
                        y=row['avg_chunk_len'],
                        text=row['chunk_strategy'],
                        showarrow=False,
                        yshift=10,  # ãƒãƒ–ãƒ«ã®ä¸Šã«é…ç½®
                        font=dict(
                            size=10,
                            family=japanese_font,
                            color='#333333'
                        ),
                        xanchor='center',
                        yanchor='bottom',
                        opacity=0.9
                    )
                
                # ç”»åƒã¨ã—ã¦ä¿å­˜ï¼ˆé«˜è§£åƒåº¦ã§ï¼‰
                current_graph += 1
                update_progress(current_graph, total_graphs, f"ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­: {model_name}")
                img_data = save_plotly_figure(fig_bubble, f"bubble_chart_{model_name}", width=1200, height=800, scale=3.0)
                if img_data:
                    filepath = os.path.join(temp_dir, f"bubble_chart_{model_name}.png")
                    with open(filepath, 'wb') as f:
                        f.write(img_data)
                    saved_files.append(filepath)
        
        # 2. ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜
        for model_name, model_data in model_groups:
            if not model_data.empty and 'chunk_strategy' in model_data.columns and 'overall_score' in model_data.columns:
                # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã”ã¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é›†è¨ˆ
                strategy_scores = model_data.groupby('chunk_strategy')['overall_score'].mean().sort_values(ascending=False)
                
                # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
                bar_data = pd.DataFrame({
                    'strategy': strategy_scores.index,
                    'score': strategy_scores.values
                })
                
                fig_bar = px.bar(
                    data_frame=bar_data,
                    x='score',
                    y='strategy',
                    orientation='h',
                    title=f"{model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                    labels={'score': 'å¹³å‡ã‚¹ã‚³ã‚¢', 'strategy': 'ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥'},
                    color='score',
                    color_continuous_scale=px.colors.sequential.Viridis,
                )
                
                # ãƒãƒ¼ã®ä¸Šã«ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                fig_bar.update_traces(
                    texttemplate='%{x:.3f}',
                    textposition='outside',
                    hovertemplate='<b>%{y}</b><br>ã‚¹ã‚³ã‚¢: %{x:.3f}<extra></extra>',
                    textfont=dict(size=12, family=japanese_font, color='#333333'),
                )
                
                # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                fig_bar.update_layout(
                    title={
                        'text': f"{model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                        'x': 0.5,
                        'xanchor': 'center',
                        'y': 0.95,
                        'yanchor': 'top',
                        'font': {
                            'size': 18,
                            'family': japanese_font,
                            'color': '#333333'
                        }
                    },
                    xaxis=dict(
                        range=[0, 1.1],
                        title=dict(
                            text='å¹³å‡ã‚¹ã‚³ã‚¢',
                            font=dict(
                                size=14,
                                family=japanese_font
                            )
                        ),
                        tickfont=dict(
                            size=12,
                            family=japanese_font
                        ),
                        showgrid=True,
                        gridwidth=1,
                        gridcolor='rgba(0, 0, 0, 0.1)',
                        showline=True,
                        linewidth=1,
                        linecolor='gray',
                        automargin=True
                    ),
                    yaxis=dict(
                        title=dict(
                            text='ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥',
                            font=dict(
                                size=14,
                                family=japanese_font
                            )
                        ),
                        tickfont=dict(
                            size=12,
                            family=japanese_font
                        ),
                        autorange="reversed",
                        automargin=True,
                        showline=True,
                        linewidth=1,
                        linecolor='gray'
                    ),
                    coloraxis_showscale=False,
                    height=500,
                    margin=dict(l=120, r=50, t=120, b=80),  # å·¦ãƒãƒ¼ã‚¸ãƒ³ã‚’å¢—ã‚„ã—ã¦ç¸¦è»¸ãƒ©ãƒ™ãƒ«ã®ãŸã‚
                    font=dict(
                        size=14,
                        family=japanese_font,
                        color='#333333'
                    ),
                    plot_bgcolor='white',
                    paper_bgcolor='white',
                    hoverlabel=dict(
                        font_size=12,
                        font_family=japanese_font
                    )
                )
                
                # ç”»åƒã¨ã—ã¦ä¿å­˜ï¼ˆé«˜è§£åƒåº¦ã§ï¼‰
                current_graph += 1
                update_progress(current_graph, total_graphs, f"ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­: {model_name}")
                img_data = save_plotly_figure(fig_bar, f"bar_chart_{model_name}", width=1200, height=800, scale=3.0)
                if img_data:
                    filepath = os.path.join(temp_dir, f"bar_chart_{model_name}.png")
                    with open(filepath, 'wb') as f:
                        f.write(img_data)
                    saved_files.append(filepath)
        
        # 3. ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ä¿å­˜
        if 'chunk_strategy' in results_df.columns:
            chunk_strategies = results_df['chunk_strategy'].unique()
            
            for strategy in chunk_strategies:
                strategy_data = results_df[results_df['chunk_strategy'] == strategy]
                
                if not strategy_data.empty:
                    fig_radar = go.Figure()
                    
                    # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    for model_name, model_data in model_groups:
                        model_strategy_data = strategy_data[strategy_data['embedding_model'] == model_name] if 'embedding_model' in strategy_data.columns else strategy_data
                        
                        if not model_strategy_data.empty:
                            # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡å€¤ã‚’è¨ˆç®—
                            r_values = [model_strategy_data[m].mean() if m in model_strategy_data.columns else 0.5 for m in metrics]
                            
                            # å„ç‚¹ã«è¡¨ç¤ºã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ï¼ˆå€¤ã¨ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼‰
                            text_values = [f'{v:.2f}' for v in r_values]
                            
                            # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã«ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿½åŠ 
                            fig_radar.add_trace(go.Scatterpolar(
                                r=r_values,
                                theta=metrics_jp,
                                fill='toself',
                                name=model_name,
                                text=text_values,
                                textposition='top center',
                                textfont=dict(
                                    size=11,
                                    color='black',
                                    family=japanese_font
                                ),
                                hovertemplate='<b>%{theta}</b><br>ã‚¹ã‚³ã‚¢: %{r:.2f}<extra></extra>',
                                line=dict(width=2),
                                mode='lines+markers+text',
                                marker=dict(
                                    size=6,
                                    opacity=0.8
                                )
                            ))
                    
                        # ã‚«ãƒ©ãƒ¼ãƒ‘ãƒ¬ãƒƒãƒˆã‚’å®šç¾©ï¼ˆè¦–èªæ€§ã®é«˜ã„è‰²ã‚’é¸æŠï¼‰
                    colors = [
                        '#1f77b4',  # é’
                        '#ff7f0e',  # ã‚ªãƒ¬ãƒ³ã‚¸
                        '#2ca02c',  # ç·‘
                        '#d62728',  # èµ¤
                        '#9467bd',  # ç´«
                        '#8c564b',  # èŒ¶è‰²
                        '#e377c2',  # ãƒ”ãƒ³ã‚¯
                        '#7f7f7f',  # ã‚°ãƒ¬ãƒ¼
                        '#bcbd22',  # ã‚ªãƒªãƒ¼ãƒ–
                        '#17becf'   # ã‚·ã‚¢ãƒ³
                    ]
                    
                    # å„ãƒˆãƒ¬ãƒ¼ã‚¹ã«è‰²ã‚’é©ç”¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤ºä½ç½®ã‚’èª¿æ•´
                    for i, trace in enumerate(fig_radar.data):
                        # å„ç‚¹ã®è§’åº¦ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ã‚’ãšã‚‰ã™
                        text_positions = []
                        for j, theta in enumerate(trace.theta):
                            # è§’åº¦ã«åŸºã¥ã„ã¦ä½ç½®ã‚’èª¿æ•´ï¼ˆ0-360åº¦ã‚’0-11ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
                            angle = (j * 30) % 360  # 30åº¦åˆ»ã¿ã§é…ç½®ï¼ˆ12æ–¹å‘ï¼‰
                            
                            # è§’åº¦ã«å¿œã˜ãŸä½ç½®ã‚’è¨­å®š
                            if 15 <= angle < 165:  # å³å´
                                pos = 'top center'
                            elif 195 <= angle < 345:  # å·¦å´
                                pos = 'bottom center'
                            else:  # ä¸Šä¸‹
                                pos = 'middle right' if angle < 180 else 'middle left'
                                
                            text_positions.append(pos)
                        
                        trace.update(
                            line=dict(
                                width=2.5,
                                color=colors[i % len(colors)]
                            ),
                            marker=dict(
                                size=8,  # ãƒãƒ¼ã‚«ãƒ¼ã‚’å°‘ã—å¤§ãã
                                color=colors[i % len(colors)],
                                line=dict(width=1, color='black'),
                                opacity=0.9
                            ),
                            textposition=text_positions,  # å‹•çš„ã«è¨­å®šã—ãŸä½ç½®ã‚’ä½¿ç”¨
                            textfont=dict(
                                size=10,  # ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’å°‘ã—å°ã•ã
                                color='black',
                                family=japanese_font
                            ),
                            mode='lines+markers+text',
                            opacity=0.8
                        )
                    
                    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                    # æˆ¦ç•¥åã‚’å–å¾—ï¼ˆæ—¢ã«ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ï¼‰
                    if 'label' in strategy_data.columns and not strategy_data.empty:
                        display_strategy = strategy_data.iloc[0]['label']
                    else:
                        # ãƒ©ãƒ™ãƒ«ãŒä»˜ä¸ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã§ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ
                        if hasattr(strategy, 'iloc'):  # pandas.Seriesã®å ´åˆ
                            strategy_value = strategy.iloc[0] if not strategy.empty else str(strategy)
                        else:
                            strategy_value = str(strategy)
                        
                        # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã‹ã‚‰åŸºæœ¬æˆ¦ç•¥åã‚’æŠ½å‡º
                        strategy_parts = str(strategy_value).strip().split('-')
                        base_strategy = strategy_parts[0].lower()
                        
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
                        print("\n=== è¡Œãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===")
                        print(f"å…ƒã®æˆ¦ç•¥å€¤: {strategy_value}")
                        print(f"æŠ½å‡ºã—ãŸåŸºæœ¬æˆ¦ç•¥: {base_strategy}")
                        
                        # åŸºæœ¬æˆ¦ç•¥ãŒã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
                        simple_strategies = ['semantic', 'sentence', 'paragraph']
                        if base_strategy in simple_strategies:
                            display_strategy = base_strategy
                            print(f"ã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã‚’æ¤œå‡º: {display_strategy}")
                        else:
                            # ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æˆ¦ç•¥ã®å ´åˆã¯ã€chunk_strategyã‚’ãã®ã¾ã¾ä½¿ç”¨
                            display_strategy = str(strategy_value).strip()
                            print(f"ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æˆ¦ç•¥ã‚’æ¤œå‡º: {display_strategy}")
                    
                    # ãƒ‡ãƒãƒƒã‚°ç”¨
                    print(f"ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆæˆ¦ç•¥åå‡¦ç† - å…ƒã®æˆ¦ç•¥: {strategy}")
                    print(f"ä½¿ç”¨ã™ã‚‹è¡¨ç¤ºæˆ¦ç•¥å: {display_strategy}")
                    
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
                    title_text = f"{display_strategy} - è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒ"
                    print(f"è¨­å®šã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«: {title_text}")
                    
                    fig_radar.update_layout(
                        title={
                            'text': title_text,
                            'x': 0.5,
                            'xanchor': 'center',
                            'y': 0.95,  # ä¸Šéƒ¨ã«ä½™ç™½ã‚’ç¢ºä¿
                            'yanchor': 'top',
                            'font': {
                                'size': 20,
                                'family': japanese_font,
                                'color': '#333333'
                            }
                        },
                        polar=dict(
                            radialaxis=dict(
                                visible=True,
                                range=[0, 1],
                                tickfont=dict(size=11, family=japanese_font, color='#555555'),
                                tickangle=0,
                                tickformat='.1f',
                                gridwidth=1,
                                gridcolor='lightgray',
                                linecolor='gray',
                                linewidth=1,
                                showline=True
                            ),
                            angularaxis=dict(
                                rotation=90,
                                direction='clockwise',
                                tickfont=dict(size=12, family=japanese_font, color='#333333'),
                                gridwidth=1,
                                gridcolor='lightgray',
                                linecolor='gray',
                                linewidth=1,
                                showline=True
                            ),
                            bgcolor='rgba(250, 250, 250, 0.8)'
                        ),
                        showlegend=True,
                        legend=dict(
                            orientation='h',
                            yanchor='top',
                            y=-0.15,  # ä¸‹å´ã«é…ç½®
                            xanchor='center',
                            x=0.5,
                            font=dict(size=12, family=japanese_font, color='#333333'),
                            bgcolor='rgba(255, 255, 255, 0.8)',
                            bordercolor='#DDDDDD',
                            borderwidth=1,
                            itemclick=False,
                            itemdoubleclick=False
                        ),
                        margin=dict(l=80, r=80, t=120, b=150),  # ä¸‹éƒ¨ã®ä½™ç™½ã‚’å¢—ã‚„ã—ã¦å‡¡ä¾‹ç”¨ã®ã‚¹ãƒšãƒ¼ã‚¹ã‚’ç¢ºä¿
                        height=600,  # é«˜ã•ã‚’å°‘ã—å¢—ã‚„ã—ã¦ä½™è£•ã‚’æŒãŸã›ã‚‹
                        paper_bgcolor='white',
                        plot_bgcolor='white',
                        font=dict(family=japanese_font, color='#333333'),
                        hoverlabel=dict(
                            font_size=12,
                            font_family=japanese_font
                        )
                    )
                    
                    # ã‚°ãƒªãƒƒãƒ‰ç·šã‚’è¿½åŠ 
                    fig_radar.update_polars(
                        radialaxis=dict(
                            showgrid=True,
                            gridcolor='lightgray',
                            gridwidth=1,
                            showline=True,
                            linecolor='gray',
                            linewidth=1
                        ),
                        angularaxis=dict(
                            showgrid=True,
                            gridcolor='lightgray',
                            gridwidth=1,
                            showline=True,
                            linecolor='gray',
                            linewidth=1
                        )
                    )
                    
                    # ç”»åƒã¨ã—ã¦ä¿å­˜ï¼ˆé«˜è§£åƒåº¦ã§ï¼‰
                    current_graph += 1
                    update_progress(current_graph, total_graphs, f"ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­: {strategy}")
                    img_data = save_plotly_figure(fig_radar, f"radar_chart_{strategy}", width=1200, height=800, scale=3.0)
                    if img_data:
                        filepath = os.path.join(temp_dir, f"radar_chart_{strategy}.png".replace("/", "_"))
                        with open(filepath, 'wb') as f:
                            f.write(img_data)
                        saved_files.append(filepath)
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        if saved_files:
            update_progress(total_graphs, total_graphs, "ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            zip_filename = f"{filename}_{timestamp}.zip"
            zip_path = os.path.join(temp_dir, zip_filename)
            
            try:
                with zipfile.ZipFile(zip_path, 'w') as zipf:
                    for i, file in enumerate(saved_files, 1):
                        zipf.write(file, os.path.basename(file))
                        update_progress(total_graphs, total_graphs, f"ZIPã«è¿½åŠ ä¸­: {i}/{len(saved_files)}")
                
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è¿”ã™
                with open(zip_path, 'rb') as f:
                    zip_data = f.read()
                
                if 'status_text' in locals():
                    status_text.success(f"å®Œäº†ï¼ {len(saved_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ZIPã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                progress_bar.empty()  # å®Œäº†ã—ãŸã‚‰ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ¶ˆã™
                return zip_data
                
            except Exception as e:
                if 'status_text' in locals():
                    status_text.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                return None
        else:
            if 'status_text' in locals():
                status_text.warning("ä¿å­˜ã™ã‚‹ã‚°ãƒ©ãƒ•ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
            
    except Exception as e:
        if 'status_text' in locals():
            status_text.error(f"ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if 'temp_dir' in locals() and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except Exception as e:
                print(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

st.set_page_config(layout="wide")
st.title("RAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")

# --- Session State Initialization ---
def init_session_state():
    if 'text' not in st.session_state:
        st.session_state.text = ""
    if 'chunks' not in st.session_state:
        st.session_state.chunks = []
    if 'evaluation_results' not in st.session_state:
        st.session_state.evaluation_results = None
    if 'bulk_evaluation_results' not in st.session_state:
        st.session_state.bulk_evaluation_results = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'llm_model' not in st.session_state:
        st.session_state.llm_model = "ollama_llama2" # Default to Ollama
    if 'embedding_model' not in st.session_state:
        st.session_state.embedding_model = "huggingface_bge_small" # Default to HuggingFace
    # --- chat_modelã‚‚llm_modelã¨åŒæœŸã—ã¦åˆæœŸåŒ– ---
    if 'chat_model' not in st.session_state:
        st.session_state.chat_model = st.session_state.llm_model


init_session_state()

# --- Backend API Calls ---
# --- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã®URLè¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã¯localhostã‚’æ¨å¥¨ï¼‰ ---
# secrets.tomlãŒå­˜åœ¨ã—ãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ä¾‹å¤–å‡¦ç†ã‚’è¿½åŠ 
# Dockerç’°å¢ƒã§ã¯backendã‚µãƒ¼ãƒ“ã‚¹åã§APIã«æ¥ç¶šã™ã‚‹ã®ãŒæ¨å¥¨
import os
try:
    BACKEND_URL = st.secrets.get('BACKEND_URL', os.environ.get('BACKEND_URL', 'http://backend:8000'))  # Dockeræ™‚ã¯backend:8000ã€ãƒ­ãƒ¼ã‚«ãƒ«æ™‚ã¯ç’°å¢ƒå¤‰æ•°orlocalhost
except Exception as e:
    print(f"[WARNING] st.secretsèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    BACKEND_URL = os.environ.get('BACKEND_URL', 'http://backend:8000')


def clear_database():
    try:
        response = requests.post(f"{BACKEND_URL}/clear_db/")
        if response.status_code == 200:
            result = response.json()
            st.success("ğŸ—‘ï¸ å…¨DBãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«å‰Šé™¤ã—ã¾ã—ãŸï¼")
            
            # å‰Šé™¤çµæœã®è©³ç´°ã‚’è¡¨ç¤º
            if "details" in result and result["details"]:
                st.write("**å‰Šé™¤çµæœè©³ç´°:**")
                for detail in result["details"]:
                    if "ã‚¨ãƒ©ãƒ¼" in detail or "å¤±æ•—" in detail:
                        st.warning(f"âš ï¸ {detail}")
                    else:
                        st.info(f"âœ… {detail}")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            st.session_state.text = ""
            st.session_state.chunks = []
            st.session_state.evaluation_results = None
            st.session_state.bulk_evaluation_results = None
            st.session_state.chat_history = []
            
            # ãƒªãƒ­ãƒ¼ãƒ‰ã‚’ä¿ƒã™
            st.info("ğŸ”„ ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦å¤‰æ›´ã‚’åæ˜ ã—ã¦ãã ã•ã„")
        else:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.text}")
    except requests.exceptions.RequestException as e:
        st.error(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
    except Exception as e:
        st.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- localStorageãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
def save_state_to_localstorage():
    state_keys = [
        "file_id", "text", "qa_questions", "qa_answers", "uploaded_file_name", "uploaded_file_bytes",
        "evaluation_results", "bulk_evaluation_results", "chunks", "chat_history",
        "current_evaluation", "evaluation_history", "active_tab",
        "tab1_content", "tab2_content", "tab3_content", "tab4_content"
    ]
    state = {}
    for k in state_keys:
        v = st.session_state.get(k)
        if v is not None:
            # ãƒã‚¤ãƒˆåˆ—ã¯base64ã§ä¿å­˜
            if k == "uploaded_file_bytes" and isinstance(v, bytes):
                state[k] = base64.b64encode(v).decode('utf-8')
            # è©•ä¾¡å±¥æ­´ã¨ã‚¿ãƒ–å†…å®¹ã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
            elif k in ["evaluation_history", "bulk_evaluation_results", "tab1_content", "tab2_content", "tab3_content", "tab4_content"]:
                state[k] = json.dumps(v)
            else:
                state[k] = v
    # JSONæ–‡å­—åˆ—ã‚’UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    js = json.dumps(state, ensure_ascii=False)
    encoded_js = base64.b64encode(js.encode('utf-8')).decode('utf-8')
    components.html(f"""
    <script>
    localStorage.setItem('rag_app_state', '{encoded_js}');
    window.parent.postMessage({{streamlitMessage: 'localStorageSaved'}}, '*');
    </script>
    """, height=0)

# --- session_stateã®åˆæœŸåŒ– ---
def init_session_state():
    default_state = {
        "file_id": None,
        "text": "",
        "qa_questions": [],
        "qa_answers": [],
        "uploaded_file_name": "",
        "uploaded_file_bytes": None,
        "evaluation_results": {},  # è©•ä¾¡çµæœ
        "bulk_evaluation_results": {},  # ãƒãƒ«ã‚¯è©•ä¾¡çµæœ
        "chunks": [],  # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        "chat_history": [],  # ãƒãƒ£ãƒƒãƒˆå±¥æ­´
        "current_evaluation": None,  # ç¾åœ¨ã®è©•ä¾¡ã‚»ãƒƒã‚·ãƒ§ãƒ³
        "evaluation_history": [],  # è©•ä¾¡å±¥æ­´
        "active_tab": "tab1",  # ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ãƒ–
        "tab1_content": {"chat_history": []},  # ã‚¿ãƒ–1ã®è¡¨ç¤ºå†…å®¹
        "tab2_content": {},  # ã‚¿ãƒ–2ã®è¡¨ç¤ºå†…å®¹
        "tab3_content": {},  # ã‚¿ãƒ–3ã®è¡¨ç¤ºå†…å®¹
        "tab4_content": {},  # ã‚¿ãƒ–4ã®è¡¨ç¤ºå†…å®¹
        "_localstorage_loaded": False
    }
    for k, v in default_state.items():
        if k not in st.session_state:
            st.session_state[k] = v

# --- UI Layout ---
with st.sidebar:
    # --- ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ ---
    # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚„ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    from streamlit_js_eval import streamlit_js_eval
    local_state = streamlit_js_eval(js_expressions="localStorage.getItem('rag_app_state')", key="check_localstorage")
    has_data = local_state is not None or any(
        st.session_state.get(k) not in [None, "", [], {}]
        for k in ["text", "qa_questions", "qa_answers", "uploaded_file_name", "uploaded_file_bytes"]
    )
    
    if has_data:
        if st.button("ãƒªã‚»ãƒƒãƒˆï¼ˆã™ã¹ã¦ã‚¯ãƒªã‚¢ï¼‰"):
            with st.spinner("ãƒªã‚»ãƒƒãƒˆã‚’å®Ÿè¡Œä¸­..."):
                # 1. localStorageã‚’ã‚¯ãƒªã‚¢
                components.html("""
                <script>
                localStorage.removeItem('rag_app_state');
                window.parent.postMessage({streamlitMessage: 'localStorageCleared'}, '*');
                </script>
                """, height=0)
                
                # 2. session_stateã‚’åˆæœŸåŒ–
                init_session_state()
                
                # 3. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢
                try:
                    response = requests.post(f"{BACKEND_URL}/clear_db/")
                    if response.status_code == 200:
                        result = response.json()
                        st.success("ğŸ—‘ï¸ ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼")
                        
                        # å‰Šé™¤çµæœã®è©³ç´°ã‚’è¡¨ç¤º
                        if "details" in result and result["details"]:
                            with st.expander("ğŸ“Š å‰Šé™¤çµæœè©³ç´°ã‚’è¡¨ç¤º"):
                                for detail in result["details"]:
                                    if "ã‚¨ãƒ©ãƒ¼" in detail or "å¤±æ•—" in detail:
                                        st.warning(f"âš ï¸ {detail}")
                                    else:
                                        st.info(f"âœ… {detail}")
                        
                        # çŠ¶æ…‹ç¢ºèª
                        st.subheader("ãƒªã‚»ãƒƒãƒˆçŠ¶æ…‹ã®ç¢ºèª")
                        col1, col2, col3, col4, col5 = st.columns(5)
                        
                        with col1:
                            st.metric("ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹", "ã‚¯ãƒªã‚¢æ¸ˆã¿")
                        with col2:
                            st.metric("ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ã‚¯ãƒªã‚¢æ¸ˆã¿")
                        with col3:
                            st.metric("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "ã‚¯ãƒªã‚¢æ¸ˆã¿")
                        with col4:
                            st.metric("è©•ä¾¡çµæœ", "ãƒªã‚»ãƒƒãƒˆæ¸ˆã¿")
                        with col5:
                            st.metric("ãƒãƒ£ãƒƒãƒˆå±¥æ­´", "ã‚¯ãƒªã‚¢æ¸ˆã¿")
                        
                        # è©³ç´°ãªçŠ¶æ…‹ã‚’è¡¨ç¤º
                        with st.expander("è©³ç´°ãªçŠ¶æ…‹ã‚’è¡¨ç¤º"):
                            st.json({
                                "session_state_text": bool(st.session_state.get("text")),
                                "session_state_chunks": len(st.session_state.get("chunks", [])),
                                "session_state_evaluation_results": bool(st.session_state.get("evaluation_results")),
                                "session_state_bulk_evaluation_results": bool(st.session_state.get("bulk_evaluation_results")),
                                "session_state_chat_history": len(st.session_state.get("chat_history", []))
                            })
                            
                    else:
                        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.text}")
                        st.stop()
                except requests.exceptions.RequestException as e:
                    st.error(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
                    st.stop()
                
                # 4. æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                st.toast("ãƒªã‚»ãƒƒãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã™...")
                time.sleep(2)  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®å¾…æ©Ÿæ™‚é–“
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰å†èª­ã¿è¾¼ã¿
                st.session_state.clear()
                st.rerun()
    else:
        st.warning("""
        ğŸ“ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“
        
        PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆã§ãã¾ã™ã€‚
        """)

    # --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ãƒœã‚¿ãƒ³ ---
    if st.button("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã¿åˆæœŸåŒ–"):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ä¸­..."):
            try:
                # 1. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢
                response = requests.post(f"{BACKEND_URL}/clear_db/")
                if response.status_code == 200:
                    result = response.json()
                    
                    # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                    st.session_state.chunks = []
                    st.session_state.evaluation_results = None
                    st.session_state.bulk_evaluation_results = None
                    st.session_state.chat_history = []
                    
                    st.success("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ­£å¸¸ã«åˆæœŸåŒ–ã—ã¾ã—ãŸï¼")
                    
                    # å‰Šé™¤çµæœã®è©³ç´°ã‚’è¡¨ç¤º
                    if "details" in result and result["details"]:
                        with st.expander("ğŸ“Š åˆæœŸåŒ–çµæœè©³ç´°ã‚’è¡¨ç¤º"):
                            for detail in result["details"]:
                                if "ã‚¨ãƒ©ãƒ¼" in detail or "å¤±æ•—" in detail:
                                    st.warning(f"âš ï¸ {detail}")
                                else:
                                    st.info(f"âœ… {detail}")
                    
                    # çŠ¶æ…‹ç¢ºèª
                    st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã®çŠ¶æ…‹ç¢ºèª")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "åˆæœŸåŒ–æ¸ˆã¿")
                    with col2:
                        st.metric("è©•ä¾¡çµæœ", "ãƒªã‚»ãƒƒãƒˆæ¸ˆã¿")
                    with col3:
                        st.metric("ãƒãƒ£ãƒƒãƒˆå±¥æ­´", "ã‚¯ãƒªã‚¢æ¸ˆã¿")
                    
                    # è©³ç´°ãªçŠ¶æ…‹ã‚’è¡¨ç¤º
                    with st.expander("è©³ç´°ãªçŠ¶æ…‹ã‚’è¡¨ç¤º"):
                        st.json({
                            "session_state_chunks": len(st.session_state.get("chunks", [])),
                            "session_state_evaluation_results": bool(st.session_state.get("evaluation_results")),
                            "session_state_bulk_evaluation_results": bool(st.session_state.get("bulk_evaluation_results")),
                            "session_state_chat_history": len(st.session_state.get("chat_history", []))
                        })
                    
                    # æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                    st.toast("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¾ã™...")
                    time.sleep(2)  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®å¾…æ©Ÿæ™‚é–“
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ã—ã¦ã‹ã‚‰å†èª­ã¿è¾¼ã¿
                    st.session_state.clear()
                    st.rerun()
                else:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.text}")
                    st.stop()
                    
            except requests.exceptions.RequestException as e:
                st.error(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
                st.error("ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.stop()

    st.header("è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’APIçµŒç”±ã§å–å¾—
    import requests
    def fetch_models():
        try:
            resp = requests.get(f"{BACKEND_URL}/list_models")
            resp.raise_for_status()
            data = resp.json()
            # ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ¥ã€…ã®ãƒªã‚¹ãƒˆã§è¿”ã™
            llm_models = data.get("LLM", [])
            embedding_models = data.get("Embedding", [])
            return {
                "llm": llm_models,
                "embedding": embedding_models
            }
        except Exception as e:
            st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"llm": [], "embedding": []}
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ä¿å­˜
    if 'models' not in st.session_state:
        st.session_state.models = fetch_models()
    
    models = st.session_state.models
    
    # LLMãƒ¢ãƒ‡ãƒ«ã®é¸æŠè‚¢
    llm_models = models.get("llm", [])
    llm_options = [m['display_name'] for m in llm_models] if llm_models else ["ollama_llama2"]
    llm_names = [m['name'] for m in llm_models] if llm_models else ["ollama_llama2"]
    
    # Embeddingãƒ¢ãƒ‡ãƒ«ã®é¸æŠè‚¢
    embedding_models = models.get("embedding", [])
    embedding_options = [m['display_name'] for m in embedding_models] if embedding_models else ["openai"]
    embedding_names = [m['name'] for m in embedding_models] if embedding_models else ["openai"]

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠãƒ­ã‚¸ãƒƒã‚¯
    default_llm_idx = 0
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³å†…llm_modelãŒ'mistral'ãªã©æœªã‚µãƒãƒ¼ãƒˆã®å ´åˆã¯è‡ªå‹•ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã«ç½®ãæ›ãˆ
    if 'llm_model' in st.session_state:
        if st.session_state.llm_model not in llm_names:
            st.session_state.llm_model = llm_names[0] if llm_names else None
        default_llm_idx = llm_names.index(st.session_state.llm_model) if st.session_state.llm_model in llm_names else 0
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    st.subheader("ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã‚’ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        st.warning("è­¦å‘Š: OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    else:
        st.sidebar.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
        
    # LLMãƒ¢ãƒ‡ãƒ«é¸æŠUI
    if llm_models:
        llm_model = st.selectbox(
            "LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
            llm_options,
            index=default_llm_idx,
            key="llm_model_select"
        )
        st.session_state.llm_model = llm_names[llm_options.index(llm_model)]
        # --- ãƒãƒ£ãƒƒãƒˆç”¨ãƒ¢ãƒ‡ãƒ«ã‚‚å¿…ãšåŒæœŸï¼ˆæœªå®šç¾©ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰ ---
        if "chat_model" not in st.session_state or st.session_state.chat_model not in llm_names:
            st.session_state.chat_model = st.session_state.llm_model
    else:
        st.warning("åˆ©ç”¨å¯èƒ½ãªLLMãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.session_state.llm_model = None
        llm_model = None
    
    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«é¸æŠè‚¢ã‚’llm_modelsã‹ã‚‰è‡ªå‹•ç”Ÿæˆ
    chat_model_options = [
        m["display_name"] if "display_name" in m else m["name"] for m in llm_models
    ] if llm_models else ["ollama_llama2"]
    chat_model_names = [
        m["name"] for m in llm_models
    ] if llm_models else ["ollama_llama2"]
    default_chat_idx = 0
    if "chat_model" in st.session_state and st.session_state.chat_model in chat_model_names:
        default_chat_idx = chat_model_names.index(st.session_state.chat_model)
    chat_model = st.selectbox(
        "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«",
        options=chat_model_options,
        index=default_chat_idx,
        key="chat_model_select"
    )
    st.session_state.chat_model = chat_model_names[chat_model_options.index(chat_model)]
    
    # Embeddingãƒ¢ãƒ‡ãƒ«é¸æŠ
    if embedding_models:
        default_emb_idx = 0
        if 'embedding_model' in st.session_state and st.session_state.embedding_model in embedding_names:
            default_emb_idx = embedding_names.index(st.session_state.embedding_model)
        
        selected_embedding = st.selectbox(
            "Embeddingãƒ¢ãƒ‡ãƒ«",
            embedding_options,
            index=default_emb_idx,
            key="embedding_model_select"
        )
        st.session_state.embedding_model = embedding_names[embedding_options.index(selected_embedding)]
    else:
        st.warning("åˆ©ç”¨å¯èƒ½ãªEmbeddingãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.session_state.embedding_model = None

    import io
    # --- file_idãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚Œã°çŠ¶æ…‹ã‚’å¾©å…ƒ ---
    # --- localStorageã‹ã‚‰å¾©å…ƒ ---
    def load_state_from_localstorage():
        from streamlit_js_eval import streamlit_js_eval
        local_state = streamlit_js_eval(js_expressions="localStorage.getItem('rag_app_state')", key="load_localstorage")
        if local_state and not st.session_state.get("_localstorage_loaded", False):
            try:
                # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèª
                if isinstance(local_state, str):
                    local_state = local_state.encode('utf-8')
                # base64ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦JSONæ–‡å­—åˆ—ã‚’å¾©å…ƒ
                js_bytes = base64.b64decode(local_state)
                js = js_bytes.decode("utf-8")
                state = json.loads(js)
                for k, v in state.items():
                    if k == "uploaded_file_bytes":
                        st.session_state[k] = base64.b64decode(v)
                    elif k in ["evaluation_history", "bulk_evaluation_results", "tab1_content", "tab2_content", "tab3_content", "tab4_content"]:
                        # è©•ä¾¡å±¥æ­´ã¨ã‚¿ãƒ–å†…å®¹ã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ‡ã‚³ãƒ¼ãƒ‰
                        st.session_state[k] = json.loads(v)
                    else:
                        st.session_state[k] = v
                st.session_state["_localstorage_loaded"] = True
            except Exception as e:
                st.warning(f"localStorageå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
                init_session_state()    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚session_stateã‚’ã‚¯ãƒªã‚¢
                init_session_state()
        # localStorageå–å¾—æ™‚ã¯window.postMessageã§å€¤ãŒè¿”ã‚‹ãŒã€Streamlitæ¨™æº–ã§ã¯ç›´æ¥å—ã‘å–ã‚Œãªã„ãŸã‚ã€
        # ã“ã“ã§ã¯ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚„å¾©å…ƒã®ãŸã³ã«save_state_to_localstorage()ã€ã‚’å‘¼ã¶ã“ã¨ã§æ°¸ç¶šåŒ–ã™ã‚‹

    load_state_from_localstorage()

    # --- LLMãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰ ---
    st.subheader("ğŸ¤– LLMãƒ¢ãƒ‡ãƒ«é¸æŠ")
    col1, col2 = st.columns(2)
    
    with col1:
        question_llm_model = st.selectbox(
            "è³ªå•ç”Ÿæˆç”¨LLMãƒ¢ãƒ‡ãƒ«",
            ["mistral", "llama3", "gpt-4o"],
            index=0,
            help="PDFã‹ã‚‰è³ªå•ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ãŸã‚ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã€‚mistralãŒé«˜é€Ÿã§æ¨å¥¨ã§ã™ã€‚"
        )
    
    with col2:
        answer_llm_model = st.selectbox(
            "å›ç­”ç”Ÿæˆç”¨LLMãƒ¢ãƒ‡ãƒ«",
            ["mistral", "llama3", "gpt-4o"],
            index=0,
            help="è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ãŸã‚ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã€‚mistralãŒé«˜é€Ÿã§æ¨å¥¨ã§ã™ã€‚"
        )

    if "file_id" in st.session_state and not st.session_state.get("text"):
        try:
            resp = requests.get(f"{BACKEND_URL}/get_extracted/{st.session_state['file_id']}")
            if resp.status_code == 200:
                data = resp.json()
                st.session_state["text"] = data.get("text", "")
                st.session_state["qa_questions"] = data.get("questions", [])
                st.session_state["qa_answers"] = data.get("answers", [])
                # --- ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒã‚¤ãƒˆåˆ—ã‚‚å¿…ãšå¾©å…ƒ ---
                st.session_state["uploaded_file_name"] = data.get("file_name", f"{st.session_state['file_id']}.pdf")
                if "pdf_bytes_base64" in data:
                    st.session_state["uploaded_file_bytes"] = base64.b64decode(data["pdf_bytes_base64"])
                st.success(f"å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ID: {st.session_state['file_id']} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã—ã¾ã—ãŸ")
                save_state_to_localstorage()
            else:
                st.warning("ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒã«å¤±æ•—ã—ã¾ã—ãŸã€‚file_idã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚")
                del st.session_state["file_id"]
        except Exception as e:
            st.warning(f"ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒã«å¤±æ•—: {e}")
            del st.session_state["file_id"]
    # ã™ã§ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ã©ã†ã‹ã§UIã‚’åˆ†å²
    if "uploaded_file_bytes" in st.session_state and "uploaded_file_name" in st.session_state:
        cleanse_used = st.session_state.get("cleanse_used", False)
        st.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆ: {st.session_state['uploaded_file_name']}ï¼ˆã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†: {'ã‚ã‚Š' if cleanse_used else 'ãªã—'}ï¼‰")
        # å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å ´åˆã®ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚„ã‚Šç›´ã™"):
            for key in ["uploaded_file_bytes", "uploaded_file_name", "uploaded_file_size", "text", "qa_questions", "qa_answers", "file_id"]:
                if key in st.session_state:
                    del st.session_state[key]
            components.html("""
            <script>localStorage.removeItem('rag_app_state');</script>
            """, height=0)
            st.rerun()
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’BytesIOã§å¾©å…ƒ
        uploaded_file = io.BytesIO(st.session_state["uploaded_file_bytes"])
        uploaded_file.name = st.session_state["uploaded_file_name"]
        # ã¾ã ãƒ†ã‚­ã‚¹ãƒˆã‚„QAãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ç„¡ã‘ã‚Œã°PDFå‡¦ç†ã‚’å®Ÿè¡Œ
        if not st.session_state.get("text"):
            # --- ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¿½åŠ  ---
            cleanse = st.checkbox("è¡¨ãƒ»ãƒã‚¤ã‚ºé™¤å»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã‚’è¡Œã†", value=False, help="PDFå†…ã®è¡¨ã‚„ãƒã‚¤ã‚ºã‚’è‡ªå‹•ã§é™¤å»ã—ã¾ã™")
            with st.spinner('PDFã‚’å‡¦ç†ä¸­...'):
                files = {'file': (uploaded_file.name, uploaded_file, 'application/pdf')}
                data = {
                    'cleanse': str(cleanse),
                    'question_llm_model': question_llm_model,
                    'answer_llm_model': answer_llm_model
                }
                try:
                    response = requests.post(f"{BACKEND_URL}/uploadfile/", files=files, data=data)
                    if response.status_code == 200:
                        data = response.json()
                        if "file_id" in data:
                            st.session_state["file_id"] = data["file_id"]
                        if 'questions' in data and 'answers' in data:
                            st.session_state.text = data['text']
                            st.session_state.qa_questions = data['questions']
                            st.session_state.qa_answers = data['answers']
                            st.session_state.qa_meta = data.get('qa_meta', [])
                            st.success("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ»è³ªå•ãƒ»å›ç­”ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã—ãŸã€‚ä»¥é™ã®è©•ä¾¡å‡¦ç†ã§ã“ã®ã‚»ãƒƒãƒˆãŒä½¿ã‚ã‚Œã¾ã™ã€‚")
                            # --- QAè¡¨ç¤ºã‚’ã‚¹ã‚³ã‚¢é †ã§æ‹¡å¼µè¡¨ç¤º ---
                            qa_meta = data.get('qa_meta', [])
                            
                            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                            st.write(f"**ãƒ‡ãƒãƒƒã‚°æƒ…å ±**: questions={len(data['questions'])}, answers={len(data['answers'])}, qa_meta={len(qa_meta)}")
                            if qa_meta:
                                st.write(f"**qa_metaã‚µãƒ³ãƒ—ãƒ«**: {qa_meta[0]}")
                            
                            # qa_metaã®é•·ã•ã‚’questions/answersã«åˆã‚ã›ã‚‹
                            if len(qa_meta) < len(data['questions']):
                                # qa_metaãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ãƒ€ãƒŸãƒ¼ã§è£œå®Œ
                                missing_count = len(data['questions']) - len(qa_meta)
                                for i in range(missing_count):
                                    qa_meta.append({
                                        'score': 1.0,
                                        'is_auto_fixed': False,
                                        'is_dummy_answer': True,
                                        'candidates': [data['answers'][len(qa_meta) + i] if len(qa_meta) + i < len(data['answers']) else 'ãƒ‡ãƒ¼ã‚¿ä¸è¶³'],
                                        'candidate_scores': [1.0]
                                    })
                                st.warning(f"qa_metaãŒ{missing_count}ä»¶ä¸è¶³ã—ã¦ã„ãŸãŸã‚ã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è£œå®Œã—ã¾ã—ãŸ")
                            
                            qa_tuples = list(zip(data['questions'], data['answers'], qa_meta))
                            # ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆ
                            qa_tuples_sorted = sorted(qa_tuples, key=lambda x: x[2].get('score', 0) if x[2] else 0, reverse=True)
                            # --- è³ªå•ç”Ÿæˆæ–¹æ³•ã®èª¬æ˜ã‚’è¿½åŠ  ---
                            with st.expander("ğŸ¤– è‡ªå‹•è³ªå•ç”Ÿæˆã®ä»•çµ„ã¿", expanded=False):
                                st.markdown("""
                                ### è³ªå•ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹
                                
                                **1. ä¸»è¦æ‰‹æ³•: LLMï¼ˆGPT-4oï¼‰ã«ã‚ˆã‚‹ç”Ÿæˆ**
                                - PDFãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã®1,500æ–‡å­—ã‚’æŠ½å‡º
                                - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: "ä»¥ä¸‹ã®å†…å®¹ã«é–¢ã™ã‚‹ä»£è¡¨çš„ãªè³ªå•ã‚’æ—¥æœ¬èªã§5ã¤ä½œæˆã—ã¦ãã ã•ã„"
                                - GPT-4oãŒæ–‡æ›¸å†…å®¹ã‚’ç†è§£ã—ã¦é©åˆ‡ãªè³ªå•ã‚’è‡ªå‹•ç”Ÿæˆ
                                
                                **2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹æ³•ï¼ˆLLMå¤±æ•—æ™‚ï¼‰**
                                - **QAå½¢å¼æŠ½å‡º**: æ—¢å­˜ã®Q&Aå½¢å¼ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è³ªå•ã‚’æŠ½å‡º
                                - **ç®‡æ¡æ›¸ãæŠ½å‡º**: ã€Œãƒ»ã€ã€Œ-ã€ã€Œ1.ã€ãªã©ã®ç®‡æ¡æ›¸ãã‚’è³ªå•åŒ–
                                - **æ®µè½è¦ç´„**: å„æ®µè½ã®å…ˆé ­æ–‡ã‹ã‚‰ã€Œã€œã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€å½¢å¼ã§ç”Ÿæˆ
                                
                                **3. å›ç­”ç”Ÿæˆ**
                                - å„è³ªå•ã«å¯¾ã—ã¦GPT-4oãŒæ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦å›ç­”ã‚’ç”Ÿæˆ
                                - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: "ä»¥ä¸‹ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ¬¡ã®è³ªå•ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„"
                                - æ–‡æ›¸ã®æœ€åˆã®3,000æ–‡å­—ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨
                                
                                **4. å“è³ªä¿è¨¼**
                                - ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå€™è£œå›ç­”é–“ã®é¡ä¼¼åº¦ï¼‰
                                - è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ï¼ˆè¤‡æ•°å€™è£œã‹ã‚‰æœ€é©å›ç­”ã‚’é¸æŠï¼‰
                                - å¿…ãšãƒ€ãƒŸãƒ¼è³ªå•ã§æœ€ä½1ä»¶ã¯ä¿è¨¼
                                
                                **5. ãƒ€ãƒŸãƒ¼å›ç­”ã®è­˜åˆ¥**
                                - ã€Œè©²å½“å†…å®¹ã‚’æœ¬æ–‡ã‹ã‚‰è¦ç´„ã—ã¦ãã ã•ã„ã€ãªã©ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ€ãƒŸãƒ¼å›ç­”ã¨åˆ¤å®š
                                - ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã®ã€Œãƒ€ãƒŸãƒ¼å›ç­”ã€ãƒãƒƒã‚¸ã§è¡¨ç¤º
                                - LLMã®å›ç­”ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆã‚„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹æ³•ã§ç”Ÿæˆ
                                """)
                            
                            st.write('### è‡ªå‹•ç”ŸæˆQAã‚»ãƒƒãƒˆï¼ˆä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢é †ï¼‰')
                            with st.expander("ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼ãƒ»èª¬æ˜", expanded=False):
                                st.markdown('''
- **ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ = å‡ºç¾å›æ•°ã‚¹ã‚³ã‚¢ + å›ç­”é•·ã‚¹ã‚³ã‚¢**
    - å‡ºç¾å›æ•°ã‚¹ã‚³ã‚¢ï¼šåŒã˜è³ªå•ãƒ»åŒã˜å›ç­”ãƒšã‚¢ãŒä½•å›å‡ºç¾ã—ãŸã‹ï¼ˆå¤šã„ã»ã©ä¿¡é ¼æ€§ãŒé«˜ã„ï¼‰
    - å›ç­”é•·ã‚¹ã‚³ã‚¢ï¼šå›ç­”ã®æ–‡å­—æ•°ã‚’å…¨ä½“ã§æ­£è¦åŒ–ï¼ˆæœ€çŸ­=0, æœ€é•·=1ï¼‰
    
**è¨ˆç®—å¼ï¼ˆPython/pandasãƒ­ã‚¸ãƒƒã‚¯ï¼‰**
```python
qa_df["count_score"] = qa_df.groupby(["question", "answer"])['answer'].transform('count')
qa_df["len_score"] = qa_df["answer"].apply(len)
qa_df["len_score"] = (qa_df["len_score"] - qa_df["len_score"].min()) / (qa_df["len_score"].max() - qa_df["len_score"].min() + 1e-6)
qa_df["total_score"] = qa_df["count_score"] + qa_df["len_score"]
```
- ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©ã€Œå¤šãå‡ºç¾ã—é•·ã„å›ç­”ã€ï¼ä¿¡é ¼æ€§ãŒé«˜ã„ã¨åˆ¤å®šã•ã‚Œã¾ã™ã€‚
- è©³ç´°ãªãƒ­ã‚¸ãƒƒã‚¯ã¯ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰`main.py`ã®è©²å½“ç®‡æ‰€ã‚’ã”å‚ç…§ãã ã•ã„ã€‚
''')
                            for idx, (q, a, meta) in enumerate(qa_tuples_sorted):
                                with st.expander(f"Q{idx+1}: {q}"):
                                    score = meta.get('score') if meta else None
                                    is_auto_fixed = meta.get('is_auto_fixed') if meta else False
                                    is_dummy_answer = meta.get('is_dummy_answer') if meta else False
                                    
                                    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
                                    st.write(f"**ãƒ‡ãƒãƒƒã‚°**: meta={meta}, is_dummy={is_dummy_answer}, is_auto_fixed={is_auto_fixed}")
                                    
                                    # ãƒãƒƒã‚¸ã®è¨­å®šï¼ˆãƒ€ãƒŸãƒ¼å›ç­”ã‚’å„ªå…ˆè¡¨ç¤ºï¼‰
                                    if is_dummy_answer:
                                        badge_text = 'ğŸŸ  ãƒ€ãƒŸãƒ¼å›ç­”'
                                        badge_color = 'orange'
                                    elif is_auto_fixed:
                                        badge_text = 'ğŸ”´ è‡ªå‹•ä¿®æ­£æ¸ˆã¿'
                                        badge_color = 'red'
                                    else:
                                        badge_text = 'ğŸ”µ ä¸€æ„å›ç­”'
                                        badge_color = 'blue'
                                    
                                    st.markdown(f"**A:** {a}")
                                    
                                    # ãƒãƒƒã‚¸ã¨ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                                    col1, col2 = st.columns([1, 3])
                                    with col1:
                                        st.markdown(f":{badge_color}[{badge_text}]")
                                    with col2:
                                        st.markdown(f"ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢: {score:.3f}" if score is not None else "ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢: -")
                                    # å€™è£œå›ç­”ãƒªã‚¹ãƒˆ
                                    candidates = meta.get('candidates', []) if meta else []
                                    candidate_scores = meta.get('candidate_scores', []) if meta else []
                                    if candidates and len(candidates) > 1:
                                        with st.expander('å€™è£œå›ç­”ãƒªã‚¹ãƒˆï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰'):
                                            for cand, cs in zip(candidates, candidate_scores):
                                                st.markdown(f"- {cand}ï¼ˆã‚¹ã‚³ã‚¢: {cs:.3f}ï¼‰")
                        else:
                            st.error(f"PDFå‡¦ç†APIã®è¿”å´å†…å®¹ã«questions/answersãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                            st.write(f"ãƒ‡ãƒãƒƒã‚°: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ = {data}")
                        save_state_to_localstorage()
                    else:
                        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {response.text}")
                except requests.exceptions.RequestException as e:
                    st.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        else:
            # æ—¢ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ»QAãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºã®ã¿
            st.success("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ»è³ªå•ãƒ»å›ç­”ã‚»ãƒƒãƒˆã¯æ—¢ã«æŠ½å‡ºæ¸ˆã¿ã§ã™ã€‚")
            # --- QAè¡¨ç¤ºã‚’ã‚¹ã‚³ã‚¢é †ã§æ‹¡å¼µè¡¨ç¤ºï¼ˆãƒªãƒ­ãƒ¼ãƒ‰å¾Œã‚‚çµ±ä¸€ï¼‰ ---
            qa_questions = st.session_state.get('qa_questions', [])
            qa_answers = st.session_state.get('qa_answers', [])
            qa_meta = st.session_state.get('qa_meta', [{}]*len(qa_questions))
            # --- qa_metaãŒç©ºã‚„é•·ã•ä¸ä¸€è‡´ãªã‚‰ãƒ€ãƒŸãƒ¼ã§è£œå®Œ ---
            if not qa_meta or len(qa_meta) != len(qa_questions):
                # ãƒ€ãƒŸãƒ¼å›ç­”ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ¤å®š
                dummy_patterns = ["è©²å½“å†…å®¹ã‚’æœ¬æ–‡ã‹ã‚‰è¦ç´„", "æœ¬æ–‡ã‚’è¦ç´„ã—ã¦"]
                qa_meta = [
                    {
                        "score": 1.0, 
                        "is_auto_fixed": False, 
                        "is_dummy_answer": any(pattern in a for pattern in dummy_patterns),
                        "candidates": [a], 
                        "candidate_scores": [1.0]
                    } 
                    for a in qa_answers
                ]
            qa_tuples = list(zip(qa_questions, qa_answers, qa_meta))
            qa_tuples_sorted = sorted(qa_tuples, key=lambda x: x[2].get('score', 0) if x[2] else 0, reverse=True)
            st.write('### è‡ªå‹•ç”ŸæˆQAã‚»ãƒƒãƒˆï¼ˆä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢é †ï¼‰')
            for idx, (q, a, meta) in enumerate(qa_tuples_sorted):
                with st.expander(f"Q{idx+1}: {q}"):
                    score = meta.get('score') if meta else None
                    is_auto_fixed = meta.get('is_auto_fixed') if meta else False
                    is_dummy_answer = meta.get('is_dummy_answer') if meta else False
                    
                    # ãƒãƒƒã‚¸ã®è¨­å®šï¼ˆãƒ€ãƒŸãƒ¼å›ç­”ã‚’å„ªå…ˆè¡¨ç¤ºï¼‰
                    if is_dummy_answer:
                        badge = ':orange[ãƒ€ãƒŸãƒ¼å›ç­”]'
                    elif is_auto_fixed:
                        badge = ':red[è‡ªå‹•ä¿®æ­£æ¸ˆã¿]'
                    else:
                        badge = ':blue[ä¸€æ„å›ç­”]'
                    
                    st.markdown(f"**A:** {a}")
                    # ã‚¹ã‚³ã‚¢ã‚’å¿…ãšæ˜ç¤ºè¡¨ç¤º
                    st.markdown(f"{badge}ï½œ<span style='color:gray'>ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢: {score:.3f}</span>" if score is not None else badge, unsafe_allow_html=True)
                    candidates = meta.get('candidates', []) if meta else []
                    candidate_scores = meta.get('candidate_scores', []) if meta else []
                    if candidates:
                        with st.expander('å€™è£œå›ç­”ãƒªã‚¹ãƒˆï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰'):
                            for cand, cs in zip(candidates, candidate_scores):
                                st.markdown(f"- {cand}ï¼ˆã‚¹ã‚³ã‚¢: {cs:.3f}ï¼‰")
        save_state_to_localstorage()
    else:
        # ã¾ã ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯file_uploaderã¨ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’è¡¨ç¤º
        cleanse = st.checkbox("è¡¨ãƒ»ãƒã‚¤ã‚ºé™¤å»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†ã‚’è¡Œã†", value=False, help="PDFå†…ã®è¡¨ã‚„ãƒã‚¤ã‚ºã‚’è‡ªå‹•ã§é™¤å»ã—ã¾ã™")
        uploaded_file = st.file_uploader("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
        if uploaded_file is not None:
            st.session_state["uploaded_file_bytes"] = uploaded_file.getvalue()
            st.session_state["uploaded_file_name"] = uploaded_file.name
            st.session_state["uploaded_file_size"] = uploaded_file.size
            st.session_state["cleanse_used"] = cleanse
            # LLMãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚‚ä¿å­˜
            st.session_state["question_llm_model"] = question_llm_model
            st.session_state["answer_llm_model"] = answer_llm_model
            save_state_to_localstorage()
            st.rerun()

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¿ãƒ–å®šç¾©
tab1, tab2, tab3, tab4, tab5 = st.tabs(["ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š", "ä¸€æ‹¬è©•ä¾¡", "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", "å’è«–å‘ã‘åˆ†æ", "è©•ä¾¡å±¥æ­´"])
tab_chatbot = tab3  # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¿ãƒ–
tab_thesis = tab4   # å’è«–å‘ã‘åˆ†æã‚¿ãƒ–
tab_history = tab5  # è©•ä¾¡å±¥æ­´ã‚¿ãƒ–

# ã‚¿ãƒ–1: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
with tab1:
    if st.session_state.text:
        st.subheader("ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š")
        # ä¸€æ‹¬è©•ä¾¡ã¨åŒã˜ãƒãƒ£ãƒ³ã‚¯æ–¹å¼é¸æŠè‚¢ã«çµ±ä¸€
        chunk_method = st.radio(
            "ãƒãƒ£ãƒ³ã‚¯åŒ–æ–¹å¼",
            ["recursive", "fixed", "semantic", "sentence", "paragraph"],
            index=0,
            help="recursive: æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹, fixed: å›ºå®šé•·, semantic: æ„å‘³ãƒ™ãƒ¼ã‚¹, sentence: æ–‡å˜ä½, paragraph: æ®µè½å˜ä½"
        )
        # semantic, sentence, paragraphã®ã¨ãã¯ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’ç„¡åŠ¹åŒ–
        size_methods = ["recursive", "fixed"]
        if chunk_method in size_methods:
            chunk_size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 200, 4000, 1000, 100, disabled=False)
            chunk_overlap = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", 0, 1000, 200, 50, disabled=False)
            similarity_threshold = 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆä½¿ã‚ãªã„ï¼‰
        else:
            chunk_size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 200, 4000, 1000, 100, disabled=True)
            chunk_overlap = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", 0, 1000, 200, 50, disabled=True)
            st.info(f"{chunk_method}æ–¹å¼ã§ã¯ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã¯è‡ªå‹•çš„ã«æ±ºå®šã•ã‚Œã¾ã™ã€‚ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®æŒ‡å®šã¯ä¸è¦ã§ã™ã€‚")
            # åˆ†å‰²ãƒ­ã‚¸ãƒƒã‚¯ã®èª¬æ˜ã‚’æ–¹å¼ã”ã¨ã«è¡¨ç¤º
            if chunk_method == "paragraph":
                st.info("ã€paragraphæ–¹å¼ã€‘1ã¤ä»¥ä¸Šã®æ”¹è¡Œï¼ˆ\\n+ï¼‰ã§æ®µè½ã”ã¨ã«åˆ†å‰²ã—ã¾ã™ã€‚ç« ã‚„æ¡æ–‡ã”ã¨ã«æ”¹è¡ŒãŒã‚ã‚Œã°è‡ªå‹•çš„ã«åŒºåˆ‡ã‚‰ã‚Œã¾ã™ã€‚")
            elif chunk_method == "sentence":
                st.info("ã€sentenceæ–¹å¼ã€‘æ—¥æœ¬èªæ–‡ã‚’spaCyï¼ˆja_core_news_smãƒ¢ãƒ‡ãƒ«ï¼‰ã§é«˜ç²¾åº¦ã«æ–‡å˜ä½ã§åˆ†å‰²ã—ã¾ã™ã€‚\n\nãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã¯è‡ªå‹•çš„ã«æ±ºå®šã•ã‚Œã¾ã™ã€‚ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®æŒ‡å®šã¯ä¸è¦ã§ã™ã€‚\n\nâ€»ä½¿ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: spaCyï¼ˆja_core_news_smï¼‰")
            elif chunk_method == "semantic":
                st.info("ã€semanticæ–¹å¼ã€‘æ„å‘³çš„ãªã¾ã¨ã¾ã‚Šã§åˆ†å‰²ã—ã¾ã™ï¼ˆembeddingé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æŒ‡å®šä¸å¯ï¼‰ã€‚\n\næ—¥æœ¬èªæ–‡ã‚’spaCyã§æ–‡å˜ä½ã«åˆ†å‰²ã—ã€å¤šè¨€èªå¯¾å¿œembeddingã§æ„å‘³çš„ãªã¾ã¨ã¾ã‚Šã‚’ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§åˆ¤å®šã—ã¾ã™ã€‚\n\nä¸‹è¨˜ã®é–¾å€¤ã‚’èª¿æ•´ã™ã‚‹ã¨ã€ãƒãƒ£ãƒ³ã‚¯ã®ç²’åº¦ãŒå¤‰ã‚ã‚Šã¾ã™ã€‚\nï¼ˆå€¤ã‚’ä¸‹ã’ã‚‹ã¨å¤§ããªãƒãƒ£ãƒ³ã‚¯ã€ä¸Šã’ã‚‹ã¨ç´°ã‹ã„ãƒãƒ£ãƒ³ã‚¯ã«ãªã‚Šã¾ã™ï¼‰")
                similarity_threshold = st.slider(
                    "é¡ä¼¼åº¦é–¾å€¤ï¼ˆsemanticåˆ†å‰²ç”¨ï¼‰",
                    min_value=0.3, max_value=0.95, value=0.7, step=0.01,
                    help="ã“ã®å€¤ã‚ˆã‚Šé¡ä¼¼åº¦ãŒä½ã„ã¨æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ãŒå§‹ã¾ã‚Šã¾ã™ï¼ˆå€¤ã‚’ä¸‹ã’ã‚‹ã¨å¤§ããªãƒãƒ£ãƒ³ã‚¯ã«ãªã‚Šã‚„ã™ã„ï¼‰"
                )
            else:
                similarity_threshold = 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆä½¿ã‚ãªã„ï¼‰
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠè‚¢ã¨ç‰¹å¾´èª¬æ˜
        embedding_models = {
            "huggingface_bge_small": "è»½é‡ãƒ¢ãƒ‡ãƒ«ã€‚ãƒªã‚½ãƒ¼ã‚¹ã«åˆ¶é™ãŒã‚ã‚‹å ´åˆã«é©ã—ã¦ã„ã¾ã™ã€‚\n- ã‚µã‚¤ã‚º: ç´„1GB\n- ç”¨é€”: ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãŒã‚ã‚‹ç’°å¢ƒã§ã®æ–‡æ›¸ç†è§£",
            "huggingface_bge_large": "é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã€‚ã‚ˆã‚Šæ­£ç¢ºãªæ–‡æ›¸ç†è§£ãŒå¯èƒ½ã§ã™ã€‚\n- ã‚µã‚¤ã‚º: ç´„8GB\n- ç”¨é€”: é«˜ç²¾åº¦ãªæ–‡æ›¸ç†è§£",
            "sentence_transformers_all-MiniLM-L6-v2": "è»½é‡ã§é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ã€‚åŸºæœ¬çš„ãªæ–‡æ›¸ç†è§£ã«é©ã—ã¦ã„ã¾ã™ã€‚\n- ã‚µã‚¤ã‚º: ç´„170MB\n- ç”¨é€”: ä¸€èˆ¬çš„ãªæ–‡æ›¸ç†è§£ã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ãŒã‚ã‚‹ç’°å¢ƒ",
            "sentence_transformers_all-mpnet-base-v2": "é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã€‚è¤‡é›‘ãªæ–‡æ›¸ç†è§£ã«é©ã—ã¦ã„ã¾ã™ã€‚\n- ã‚µã‚¤ã‚º: ç´„420MB\n- ç”¨é€”: é«˜ç²¾åº¦ãªæ–‡æ›¸ç†è§£ã€è¤‡é›‘ãªæ–‡è„ˆç†è§£",
            "sentence_transformers_multi-qa-MiniLM-L6-cos-v1": "QAã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚è³ªå•å¿œç­”ã®ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚\n- ã‚µã‚¤ã‚º: ç´„170MB\n- ç”¨é€”: è³ªå•å¿œç­”ã®ç²¾åº¦ã‚’é‡è¦–ã™ã‚‹å ´åˆ",
            "sentence_transformers_multi-qa-MiniLM-L6-dot-v1": "QAã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‰ãƒƒãƒˆç©ç‰ˆï¼‰ã€‚\n- ã‚µã‚¤ã‚º: ç´„170MB\n- ç”¨é€”: è³ªå•å¿œç­”ã®ç²¾åº¦ã‚’é‡è¦–ã™ã‚‹å ´åˆï¼ˆãƒ‰ãƒƒãƒˆç©ç‰ˆï¼‰",
            "sentence_transformers_multi-qa-mpnet-base-dot-v1": "é«˜æ€§èƒ½ãªQAç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„420MB\n- ç”¨é€”: é«˜ç²¾åº¦ãªè³ªå•å¿œç­”ãŒå¿…è¦ãªå ´åˆ",
            "sentence_transformers_paraphrase-multilingual-MiniLM-L12-v2": "å¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„350MB\n- ç”¨é€”: å¤šè¨€èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†",
            "sentence_transformers_paraphrase-MiniLM-L6-v2": "è»½é‡ãªå¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„170MB\n- ç”¨é€”: è»½é‡ãªå¤šè¨€èªå¯¾å¿œãŒå¿…è¦ãªå ´åˆ",
            "sentence_transformers_paraphrase-multilingual-mpnet-base-v2": "é«˜æ€§èƒ½ãªå¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„420MB\n- ç”¨é€”: é«˜ç²¾åº¦ãªå¤šè¨€èªå¯¾å¿œãŒå¿…è¦ãªå ´åˆ",
            "sentence_transformers_distiluse-base-multilingual-cased-v2": "è»½é‡ãªæ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„170MB\n- ç”¨é€”: æ—¥æœ¬èªæ–‡æ›¸ã®å‡¦ç†ã«é©ã—ã¦ã„ã¾ã™",
            "sentence_transformers_distiluse-base-multilingual-cased-v1": "è»½é‡ãªå¤šè¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„170MB\n- ç”¨é€”: è»½é‡ãªå¤šè¨€èªå¯¾å¿œãŒå¿…è¦ãªå ´åˆ",
            "sentence_transformers_xlm-r-100langs-bert-base-nli-stsb-mean-tokens": "100è¨€èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„1.2GB\n- ç”¨é€”: 100ä»¥ä¸Šã®è¨€èªã‚’å‡¦ç†ã™ã‚‹å ´åˆ",
            "microsoft_layoutlm-base-uncased": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è€ƒæ…®ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚PDFãªã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã«é©ã—ã¦ã„ã¾ã™ã€‚\n- ã‚µã‚¤ã‚º: ç´„420MB\n- ç”¨é€”: PDFãªã©ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’è€ƒæ…®ã—ãŸå‡¦ç†",
            "microsoft_layoutlmv3-base": "ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±ã‚‚è€ƒæ…®ã—ãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã€‚\n- ã‚µã‚¤ã‚º: ç´„1.2GB\n- ç”¨é€”: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæƒ…å ±ã‚‚è€ƒæ…®ã—ãŸé«˜ç²¾åº¦ãªå‡¦ç†"
        }
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠUI
        selected_model = st.selectbox(
            "åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« (semanticæ™‚å¿…é ˆ)", 
            list(embedding_models.keys()),
            format_func=lambda x: f"{x} - {embedding_models[x]}",
            index=0)
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´ã‚’è¡¨ç¤º
        st.write(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´: {embedding_models[selected_model]}")
        embedding_model = selected_model
        
        if st.button("ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–"):
            with st.spinner('ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œä¸­...'):
                # 1. Chunk
                payload = {
                    "text": st.session_state.text,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "chunk_method": chunk_method,
                }
                if chunk_method == "semantic":
                    payload["embedding_model"] = embedding_model
                    payload["similarity_threshold"] = similarity_threshold

                chunk_response = requests.post(f"{BACKEND_URL}/chunk/", json=payload)
                if chunk_response.status_code == 200:
                    st.session_state.chunks = chunk_response.json()['chunks']
                    # 2. Embed
                    embed_payload = {
                        "chunks": st.session_state.chunks,
                        "embedding_model": st.session_state.embedding_model,
                        "chunk_method": chunk_method  # ãƒãƒ£ãƒ³ã‚¯æ–¹å¼ã‚’è¿½åŠ 
                    }
                    embed_response = requests.post(f"{BACKEND_URL}/embed_and_store/", json=embed_payload)
                    if embed_response.status_code == 200:
                        st.success(f"{len(st.session_state.chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆã—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã—ãŸã€‚")
                    else:
                        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {embed_response.text}")
                else:
                    st.error(f"ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ: {chunk_response.text}")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
if not st.session_state.text:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€è¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")
    st.stop()

# ã‚¿ãƒ–2: ä¸€æ‹¬è©•ä¾¡
with tab2:
    st.header("ä¸€æ‹¬è©•ä¾¡")
    st.markdown("Embeddingãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹å¼ãƒ»ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å…¨çµ„ã¿åˆã‚ã›ã§ä¸€æ‹¬è‡ªå‹•è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚")
    
    # ã‚¹ã‚³ã‚¢ã®èª¬æ˜ã‚’è¡¨ç¤º
    with st.expander("è©•ä¾¡æŒ‡æ¨™ã®èª¬æ˜", expanded=True):
        st.markdown("""
        ### è©•ä¾¡æŒ‡æ¨™ã®èª¬æ˜
        
        | æŒ‡æ¨™ | èª¬æ˜ | ç†æƒ³å€¤ |
        |------|------|------|
        | **Faithfulness (ä¿¡é ¼æ€§)** | ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’æ¸¬å®š | 1.0 |
        | **Answer Relevancy (å›ç­”ã®é–¢é€£æ€§)** | å›ç­”ãŒè³ªå•ã¨ã©ã‚Œã ã‘é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’æ¸¬å®š | 1.0 |
        | **Context Recall (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§)** | é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®æƒ…å ±ãŒæ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®š | 1.0 |
        | **Context Precision (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§)** | æ¤œç´¢çµæœã®ã†ã¡ã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒã©ã‚Œã ã‘æ­£ç¢ºã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®š | 1.0 |
        | **Answer Correctness (å›ç­”ã®æ­£ç¢ºæ€§)** | å›ç­”ã®äº‹å®Ÿé–¢ä¿‚ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’æ¸¬å®š | 1.0 |
        | **Overall Score (ç·åˆã‚¹ã‚³ã‚¢)** | ä¸Šè¨˜ã®ã‚¹ã‚³ã‚¢ã‚’å¹³å‡ã—ãŸç·åˆçš„ãªè©•ä¾¡å€¤ | 1.0 |
        
        **æ³¨:** ã™ã¹ã¦ã®ã‚¹ã‚³ã‚¢ã¯0ã€œ1ã®ç¯„å›²ã§æ­£è¦åŒ–ã•ã‚Œã¦ãŠã‚Šã€1ã«è¿‘ã„ã»ã©è‰¯ã„çµæœã‚’ç¤ºã—ã¾ã™ã€‚
        """)

    # Embeddingãƒ¢ãƒ‡ãƒ«ã®è¤‡æ•°é¸æŠ
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
    if 'models' not in st.session_state:
        st.error("ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
        
    embedding_models = st.session_state.models.get("embedding", [])
    
    # ãƒ¢ãƒ‡ãƒ«åã¨è¡¨ç¤ºåã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    embedding_options = {}
    for model in embedding_models:
        model_id = model.get("name", "")  # model_idã§ã¯ãªãnameã‚’ä½¿ç”¨
        model_name = model.get("display_name", model_id)  # è¡¨ç¤ºåãŒãªã‘ã‚Œã°model_idã‚’ä½¿ç”¨
        provider = model.get("type", "").lower()  # providerã§ã¯ãªãtypeã‚’ä½¿ç”¨
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸæ¥é ­è¾ã‚’è¿½åŠ 
        if "openai" in provider:
            display_name = f"OpenAI: {model_name}"
        elif "huggingface" in provider:
            display_name = f"HuggingFace: {model_name}"
        else:
            display_name = f"{provider}: {model_name}"
            
        embedding_options[display_name] = model_id
    
    # ãƒ¢ãƒ‡ãƒ«ãŒ1ã¤ã‚‚ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    if not embedding_options:
        st.error("åˆ©ç”¨å¯èƒ½ãªEmbeddingãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    default_selection = [list(embedding_options.values())[0]]
    
    # ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆã§ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    selected_embeddings = st.multiselect(
        "Embeddingãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
        options=list(embedding_options.values()),
        format_func=lambda x: [k for k, v in embedding_options.items() if v == x][0],
        default=default_selection,
        key="bulk_embeddings_tab2"
    )

    # LLMãƒ¢ãƒ‡ãƒ«ã®é¸æŠã‚’è¿½åŠ 
    st.subheader("LLMãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    # LLMãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
    llm_models = st.session_state.models.get("llm", [])
    
    # LLMãƒ¢ãƒ‡ãƒ«åã¨è¡¨ç¤ºåã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    llm_options = {}
    for model in llm_models:
        model_id = model.get("name", "")
        model_name = model.get("display_name", model_id)
        provider = model.get("type", "").lower()
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«å¿œã˜ãŸæ¥é ­è¾ã‚’è¿½åŠ 
        if "openai" in provider:
            display_name = f"OpenAI: {model_name}"
        elif "ollama" in provider or model_id in ["mistral", "llama3", "ollama_llama2"]:
            display_name = f"Ollama: {model_name}"
        else:
            display_name = f"{provider}: {model_name}"
            
        llm_options[display_name] = model_id
    
    # LLMãƒ¢ãƒ‡ãƒ«ãŒ1ã¤ã‚‚ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    if not llm_options:
        st.error("åˆ©ç”¨å¯èƒ½ãªLLMãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§Mistralã‚’é¸æŠï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    default_llm = "mistral" if "mistral" in llm_options.values() else list(llm_options.values())[0]
    
    # LLMãƒ¢ãƒ‡ãƒ«é¸æŠ
    selected_llm_model = st.selectbox(
        "ä¸€æ‹¬è©•ä¾¡ã§ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«",
        options=list(llm_options.values()),
        format_func=lambda x: [k for k, v in llm_options.items() if v == x][0],
        index=list(llm_options.values()).index(default_llm) if default_llm in llm_options.values() else 0,
        key="bulk_llm_model",
        help="ä¸€æ‹¬è©•ä¾¡ã§è³ªå•ãƒ»å›ç­”ç”Ÿæˆã¨RAGASè©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
    )

    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹æ³•ã®é¸æŠ
    chunk_methods = st.multiselect(
        "ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹æ³• (è¤‡æ•°é¸æŠå¯)",
        options=["recursive", "fixed", "semantic", "sentence", "paragraph"],
        default=["recursive"],
        help="è¤‡æ•°é¸æŠã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹æ³•ã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚"
    )
    
    # ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¿…è¦ã¨ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ–¹æ³•ã‚’å®šç¾©
    NEEDS_SIZE_OVERLAP = ["recursive", "fixed"]
    
    # é¸æŠã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯æ–¹æ³•ã‹ã‚‰ã€ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    needs_size_overlap = any(method in chunk_methods for method in NEEDS_SIZE_OVERLAP)
    has_semantic = "semantic" in chunk_methods
    # semanticæ–¹å¼é¸æŠæ™‚ã®ã¿é–¾å€¤ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
    if has_semantic:
        st.info("semanticæ–¹å¼ã§ã¯ã€embeddingã«ã‚ˆã‚‹æ„å‘³çš„åˆ†å‰²ã®é–¾å€¤ã‚’èª¿æ•´ã§ãã¾ã™ã€‚\nå€¤ã‚’ä¸‹ã’ã‚‹ã¨å¤§ããªãƒãƒ£ãƒ³ã‚¯ã€ä¸Šã’ã‚‹ã¨ç´°ã‹ã„ãƒãƒ£ãƒ³ã‚¯ã«ãªã‚Šã¾ã™ã€‚")
        bulk_similarity_threshold = st.slider(
            "é¡ä¼¼åº¦é–¾å€¤ï¼ˆsemanticåˆ†å‰²ç”¨ï¼‰",
            min_value=0.3, max_value=0.95, value=0.7, step=0.01,
            help="ã“ã®å€¤ã‚ˆã‚Šé¡ä¼¼åº¦ãŒä½ã„ã¨æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ãŒå§‹ã¾ã‚Šã¾ã™ï¼ˆå€¤ã‚’ä¸‹ã’ã‚‹ã¨å¤§ããªãƒãƒ£ãƒ³ã‚¯ã«ãªã‚Šã‚„ã™ã„ï¼‰",
            key="bulk_similarity_threshold"
        )
    else:
        bulk_similarity_threshold = 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®é¸æŠï¼ˆå¿…è¦ãªå ´åˆã®ã¿æœ‰åŠ¹åŒ–ï¼‰
    chunk_sizes = st.multiselect(
        "ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰",
        [128, 256, 500, 1000, 1500, 2000],
        default=[500, 1000] if needs_size_overlap else [1000],
        disabled=not needs_size_overlap,
        help="recursive/fixedãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã®å ´åˆã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
    )
    
    # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®é¸æŠï¼ˆå¿…è¦ãªå ´åˆã®ã¿æœ‰åŠ¹åŒ–ï¼‰
    chunk_overlaps = st.multiselect(
        "ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰",
        [0, 32, 64, 100, 200, 300],
        default=[0, 100] if needs_size_overlap else [0],
        disabled=not needs_size_overlap,
        help="recursive/fixedãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã®å ´åˆã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
    )
    
    # æƒ…å ±è¡¨ç¤º
    # ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¿…è¦ã¨ã—ãªã„ãƒãƒ£ãƒ³ã‚¯æ–¹æ³•
    non_size_methods = ["semantic", "sentence", "paragraph"]
    selected_non_size_methods = [m for m in chunk_methods if m in non_size_methods]
    
    if selected_non_size_methods and needs_size_overlap:
        methods_text = "ã€".join(selected_non_size_methods)
        st.info(f"{methods_text}ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯æ„å‘³çš„ãªã¾ã¨ã¾ã‚Šã§åˆ†å‰²ã•ã‚Œã‚‹ãŸã‚ã€ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å½±éŸ¿ã‚’å—ã‘ã¾ã›ã‚“ã€‚")
    elif selected_non_size_methods:
        methods_text = "ã€".join(selected_non_size_methods)
        st.info(f"{methods_text}ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯æ„å‘³çš„ãªã¾ã¨ã¾ã‚Šã§åˆ†å‰²ã•ã‚Œã‚‹ãŸã‚ã€ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã¯ä¸è¦ã§ã™ã€‚")
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®šï¼ˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
    if not chunk_sizes:
        chunk_sizes = [1000]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    if not chunk_overlaps:
        chunk_overlaps = [0]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    st.caption("â€»Embeddingãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹å¼ãƒ»ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å…¨çµ„ã¿åˆã‚ã›ã§è‡ªå‹•ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™")

    if st.button("ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œ", key="bulk_evaluate_button_2"):
        # ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not st.session_state.get("text"):
            st.error("è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ã¾ãšãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.stop()
            
        with st.spinner("ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œä¸­..."):
            import concurrent.futures
            from concurrent.futures import ThreadPoolExecutor
            import time
            
            bulk_results = []
            invalid_combinations = []
            valid_combinations = []
            
            # æœ‰åŠ¹ãªçµ„ã¿åˆã‚ã›ã‚’ç”Ÿæˆ
            for method in chunk_methods:
                if method in NEEDS_SIZE_OVERLAP:
                    # ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¿…è¦ã¨ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ–¹æ³•
                    for size in chunk_sizes:
                        for overlap in chunk_overlaps:
                            if size > overlap:
                                valid_combinations.append((method, size, overlap))
                            else:
                                invalid_combinations.append((method, size, overlap))
                else:
                    # ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’å¿…è¦ã¨ã—ãªã„ãƒãƒ£ãƒ³ã‚¯æ–¹æ³•
                    # ã“ã‚Œã‚‰ã®æ–¹æ³•ã§ã¯ã‚µã‚¤ã‚º/ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã¯ç„¡è¦–ã•ã‚Œã‚‹
                    valid_combinations.append((method, None, None))
            
            if not valid_combinations:
                st.error("æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã®çµ„ã¿åˆã‚ã›ãŒã‚ã‚Šã¾ã›ã‚“ã€‚chunk_size > overlap ã¨ãªã‚‹ã‚ˆã†ã«é¸æŠã—ã¦ãã ã•ã„ã€‚")
                st.stop()
            
            if invalid_combinations:
                st.warning(f"chunk_size <= overlap ã¨ãªã‚‹ä¸æ­£ãªçµ„ã¿åˆã‚ã›ã¯è‡ªå‹•çš„ã«é™¤å¤–ã—ã¾ã—ãŸ: {invalid_combinations}")
            
            # é€²æ—ãƒãƒ¼ã€ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºã®è¨­å®š
            progress_bar = st.progress(0)
            progress_text = st.empty()
            status_display = st.empty()
            total_tasks = len(selected_embeddings) * len(valid_combinations)
            
            # å®Œäº†ã‚¿ã‚¹ã‚¯æ•°ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆï¼ˆãƒŸãƒ¥ãƒ¼ã‚¿ãƒ–ãƒ«ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
            completed_tasks = [0]
            
            # è©•ä¾¡ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
            def evaluate_single(emb, method, size, overlap):
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆã®å­˜åœ¨ã‚’å†ç¢ºèª
                    text = st.session_state.get("text")
                    qa_questions = st.session_state.get("qa_questions", [])
                    qa_answers = st.session_state.get("qa_answers", [])
                    
                    if not text:
                        st.error("è©•ä¾¡å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                        return None
                        
                    if not qa_questions or not qa_answers:
                        st.error("è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€Q&Aã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                        return None
                        
                    payload = {
                        "embedding_model": emb,
                        "llm_model": selected_llm_model,  # ä¸€æ‹¬è©•ä¾¡ç”¨LLMãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
                        "chunk_methods": [method],
                        "chunk_sizes": [size] if size is not None else [1000],
                        "chunk_overlaps": [overlap] if overlap is not None else [200],
                        "text": text,
                        "questions": qa_questions,
                        "answers": qa_answers,
                    }
                    
                    if method == "semantic":
                        payload["similarity_threshold"] = bulk_similarity_threshold
                    
                    response = requests.post(
                        f"{BACKEND_URL}/bulk_evaluate/", 
                        json=payload, 
                        timeout=300
                    )
                    
                    completed_tasks[0] += 1
                    progress_bar.progress(min(completed_tasks[0] / total_tasks, 1.0))
                    progress_text.text(f"å®Œäº†: {completed_tasks[0]} / {total_tasks} ä»¶")
                    
                    if response.status_code == 200:
                        result = response.json()
                        
                        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒªã‚¹ãƒˆã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’å–å¾—
                        if isinstance(result, list):
                            if len(result) > 0:
                                result = result[0]
                            else:
                                result = {}
                        
                        # çµæœãŒè¾æ›¸ã§ãªã„å ´åˆã¯è¾æ›¸ã«å¤‰æ›
                        if not isinstance(result, dict):
                            result = {}
                        
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                        result['embedding_model'] = emb
                        result['llm_model'] = selected_llm_model  # LLMãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’è¿½åŠ 
                        result['chunk_method'] = method
                        result['chunk_size'] = size
                        result['chunk_overlap'] = overlap
                        
                        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                        if 'overlap' not in result:
                            result['overlap'] = overlap if overlap is not None else 0
                            
                        return result
                    else:
                        st.error(f"è©•ä¾¡ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {response.text}")
                        return None
                        
                except requests.exceptions.RequestException as e:
                    st.error(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return None
                except json.JSONDecodeError as e:
                    st.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return None
                except Exception as e:
                    st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    import traceback
                    st.text(traceback.format_exc())
                    return None
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ä¸¦åˆ—å‡¦ç†ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
            bulk_results = []
            
            try:
                for emb in selected_embeddings:
                    for method, size, overlap in valid_combinations:
                        # ç¾åœ¨ã®è©•ä¾¡ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤º
                        status_display.info(f"è©•ä¾¡ä¸­: LLM={selected_llm_model}, Emb={emb}, {method}, size={size}, overlap={overlap}")
                        
                        # è©•ä¾¡ã‚’å®Ÿè¡Œ
                        result = evaluate_single(emb, method, size, overlap)
                        
                        if result:
                            bulk_results.append(result)
                            status_display.success(f"å®Œäº†: LLM={selected_llm_model}, Emb={emb}, {method}, size={size}, overlap={overlap}")
                        else:
                            status_display.warning(f"ã‚¹ã‚­ãƒƒãƒ—: LLM={selected_llm_model}, Emb={emb}, {method}, size={size}, overlap={overlap}")
                        
                        # å°‘ã—å¾…æ©Ÿï¼ˆUIã®æ›´æ–°ã®ãŸã‚ï¼‰
                        import time
                        time.sleep(0.1)
                
                # æœ€çµ‚çš„ãªé€²æ—è¡¨ç¤º
                progress_text.success(f"å®Œäº†: {total_tasks} / {total_tasks} ä»¶")
                
                # é€²æ—ãƒãƒ¼ã‚’100%ã«
                progress_bar.progress(1.0)
                
                # æœ€çµ‚çš„ãªã‚µãƒãƒªã‚’è¡¨ç¤º
                if bulk_results:
                    status_display.success(f"è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼ (æˆåŠŸ: {len(bulk_results)}ä»¶)")
                else:
                    status_display.error("è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸãŒã€æœ‰åŠ¹ãªçµæœã¯å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            except Exception as e:
                status_display.error(f"è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            
            # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            if bulk_results:
                try:
                    # çµæœãŒãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã®å ´åˆã«ãƒ•ãƒ©ãƒƒãƒˆåŒ–
                    flat_results = []
                    for result in bulk_results:
                        if isinstance(result, list):
                            flat_results.extend(result)
                        else:
                            flat_results.append(result)
                    
                    st.session_state.bulk_evaluation_results = flat_results
                    
                    # çµæœã®è©³ç´°ã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
                    if bulk_results:
                        print("\n=== è©•ä¾¡çµæœã‚µãƒãƒª ===")
                        print(f"æˆåŠŸ: {len(bulk_results)}ä»¶")
                        for i, result in enumerate(bulk_results, 1):
                            print(f"\n--- çµæœ {i} ---")
                            print(json.dumps(result, ensure_ascii=False, indent=2))
                except Exception as e:
                    if 'status_text' in locals():
                        status_text.error(f"çµæœã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")


    if st.session_state.bulk_evaluation_results:
        st.subheader("ä¸€æ‹¬è©•ä¾¡çµæœ")
        st.write("ä¸€æ‹¬è©•ä¾¡APIã®è¿”å´å†…å®¹:", st.session_state.bulk_evaluation_results)  # è¿”å´å†…å®¹ã‚’ç¢ºèªç”¨ã«è¡¨ç¤º
        
        # çµæœã‚’DataFrameã«å¤‰æ›
        eval_results = st.session_state.bulk_evaluation_results
        
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ
        if isinstance(eval_results, list):
            results_df = pd.DataFrame(eval_results)
        else:
            results_df = pd.DataFrame([eval_results])
            
        # ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’å±•é–‹
        if 'scores' in results_df.columns:
            # ã‚¹ã‚³ã‚¢ãŒè¾æ›¸å½¢å¼ã§æ ¼ç´ã•ã‚Œã¦ã„ã‚‹å ´åˆã€å„ã‚¹ã‚³ã‚¢ã‚’å€‹åˆ¥ã®ã‚«ãƒ©ãƒ ã«å±•é–‹
            scores_df = pd.json_normalize(results_df['scores'])
            # å…ƒã®ã‚«ãƒ©ãƒ ã¨çµåˆï¼ˆæ¥é ­è¾ã‚’ä»˜ã‘ã¦ç«¶åˆã‚’é¿ã‘ã‚‹ï¼‰
            results_df = pd.concat([results_df.drop('scores', axis=1), scores_df.add_prefix('score_')], axis=1)
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®å¤‰æ›
        numeric_cols = ['avg_chunk_len', 'num_chunks', 'overall_score', 'faithfulness', 
                       'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness']
        for col in numeric_cols:
            if col in results_df.columns:
                results_df[col] = pd.to_numeric(results_df[col], errors='coerce')
        
        # å¿…è¦ã‚«ãƒ©ãƒ è£œå®Œãƒ»ãƒ©ãƒ™ãƒ«åˆ—è¿½åŠ 
        required_cols = {
            'avg_chunk_len', 'num_chunks', 'overall_score', 'chunk_strategy', 'embedding_model', 'llm_model',
            'faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness'
        }
        
        # ã‚¹ã‚³ã‚¢ã‚«ãƒ©ãƒ ã®è£œå®Œï¼ˆscore_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒä»˜ã„ã¦ã„ã‚‹å ´åˆã«å¯¾å¿œï¼‰
        for col in ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness', 'overall_score']:
            if f'score_{col}' in results_df.columns and col not in results_df.columns:
                results_df[col] = results_df[f'score_{col}']
        
        # chunk_methodã‚’chunk_strategyã¨ã—ã¦ä½¿ç”¨ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        if 'chunk_method' in results_df.columns and 'chunk_strategy' not in results_df.columns:
            results_df['chunk_strategy'] = results_df['chunk_method']
        
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æƒ…å ±ã‚’è¿½åŠ ï¼ˆchunk_overlapã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦ä½¿ç”¨ï¼‰
        if 'overlap' not in results_df.columns and 'chunk_overlap' in results_df.columns:
            results_df['overlap'] = results_df['chunk_overlap']
        
        # ä¸è¶³ã‚«ãƒ©ãƒ ã®è£œå®Œ
        missing_cols = required_cols - set(results_df.columns)
        if missing_cols:
            st.info(f'ä¸è¶³ã—ã¦ã„ã‚‹ã‚«ãƒ©ãƒ ã‚’è£œå®Œã—ã¾ã™: {missing_cols}')
            for col in missing_cols:
                if col in ['chunk_strategy', 'embedding_model', 'llm_model']:
                    results_df[col] = 'unknown'
                else:
                    results_df[col] = 0.0
        
        # é‡è¤‡ã‚’å‰Šé™¤ï¼ˆåŒã˜chunk_strategyã€embedding_modelã€llm_modelã®çµ„ã¿åˆã‚ã›ã§æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä¿æŒï¼‰
        results_df = results_df.drop_duplicates(
            subset=['chunk_strategy', 'embedding_model', 'llm_model'], 
            keep='first'
        )
        
        # ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        def create_label(row):
            chunk_strategy = str(row.get('chunk_strategy', 'unknown')).strip()
            strategy_parts = chunk_strategy.split('-')
            base_strategy = strategy_parts[0].lower()
            
            # ã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã®å ´åˆã¯åŸºæœ¬åã®ã¿è¿”ã™
            if base_strategy in ['semantic', 'sentence', 'paragraph']:
                return base_strategy
                
            # æœªçŸ¥ã®æˆ¦ç•¥ã®å‡¦ç†
            if base_strategy == 'unknown':
                return 'unknown'
                
            # ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æˆ¦ç•¥ã®å‡¦ç†
            try:
                chunk_size = int(float(row.get('chunk_size', 0)))
                chunk_overlap = int(float(row.get('chunk_overlap', 0)))
                
                if chunk_size > 0:
                    return f"{base_strategy}-{chunk_size}-{chunk_overlap}"
                return base_strategy
                    
            except (ValueError, TypeError):
                return base_strategy
        
        # ãƒ©ãƒ™ãƒ«åˆ—ã‚’è¿½åŠ 
        results_df['label'] = results_df.apply(create_label, axis=1)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
        if 'debug' in st.session_state and st.session_state.debug:
            print("\n=== å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  ===")
            print(results_df[['chunk_strategy', 'chunk_size', 'chunk_overlap', 'label']].to_string())
        
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æ¯”è¼ƒã‚’è¡¨ç¤º
        if 'overlap' in results_df.columns and len(results_df['overlap'].unique()) > 1:
            plot_overlap_comparison(results_df)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’å®šç¾©
            metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision", "answer_correctness"]
            metrics_jp = ["ä¿¡é ¼æ€§", "å›ç­”ã®é–¢é€£æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§", "å›ç­”ã®æ­£ç¢ºæ€§"]
            
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if 'embedding_model' in results_df.columns:
                model_groups = list(results_df.groupby('embedding_model'))
            else:
                model_groups = [('default', results_df)]

            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
            for model_name, model_data in model_groups:
                if not model_data.empty and 'chunk_size' in model_data.columns and 'overall_score' in model_data.columns:
                    # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
                    # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                    if 'num_chunks' not in model_data.columns:
                        model_data['num_chunks'] = 0
                    if 'avg_chunk_len' not in model_data.columns:
                        model_data['avg_chunk_len'] = 0
                    if 'overall_score' not in model_data.columns:
                        model_data['overall_score'] = 0
                    
                    # ãƒãƒ–ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—ï¼ˆ0é™¤ç®—ã‚’é˜²ãï¼‰
                    bubble_sizes = [min(s * 20, 50) if pd.notnull(s) else 5 for s in model_data["overall_score"]]
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆå…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ãªã„ã‚ˆã†ã«ï¼‰
                    plot_data = model_data.copy()
                    plot_data['bubble_size'] = bubble_sizes
                    
                    # æˆ¦ç•¥åã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                    def safe_strategy_format(x):
                        if not isinstance(x, str):
                            return x
                        try:
                            prefix = x.split('-')[0].lower()
                            return prefix if prefix in ['semantic', 'sentence', 'paragraph'] else x
                        except (AttributeError, IndexError):
                            return x
                            
                    plot_data['formatted_strategy'] = plot_data['chunk_strategy'].apply(safe_strategy_format)
                    
                    # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®èª¬æ˜ã‚’è¡¨ç¤º
                    with st.expander(f"{model_name} - ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®è¦‹æ–¹", expanded=False):
                        st.markdown(f"""
                        ### ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®è¦‹æ–¹
                        - **Xè»¸**: ãƒãƒ£ãƒ³ã‚¯æ•° - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã„ãã¤ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚ŒãŸã‹
                        - **Yè»¸**: å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º - 1ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®å¹³å‡æ–‡å­—æ•°
                        - **ãƒãƒ–ãƒ«ã®ã‚µã‚¤ã‚º**: ç·åˆã‚¹ã‚³ã‚¢ã«åŸºã¥ãï¼ˆã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©å¤§ãã„ï¼‰
                        - **ãƒãƒ–ãƒ«ã®è‰²**: ç·åˆã‚¹ã‚³ã‚¢ï¼ˆé’ã«è¿‘ã„ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ã„ï¼‰
                        
                        ### ã‚¹ã‚³ã‚¢ã®ç®—å‡ºæ–¹æ³•
                        - **ç·åˆã‚¹ã‚³ã‚¢ (Overall Score)**:
                          ```
                          ç·åˆã‚¹ã‚³ã‚¢ = (faithfulness + answer_relevancy + context_recall + context_precision + answer_correctness) / 5
                          ```
                          å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯0ã€œ1ã®å€¤ã‚’å–ã‚Šã€1ã«è¿‘ã„ã»ã©è‰¯ã„çµæœã§ã™ã€‚
                          
                        - **ãƒãƒ–ãƒ«ã‚µã‚¤ã‚ºã®è¨ˆç®—**:
                          ```
                          ãƒãƒ–ãƒ«ã‚µã‚¤ã‚º = min(ç·åˆã‚¹ã‚³ã‚¢ * 20, 50)
                          ```
                          ï¼ˆæœ€å°ã‚µã‚¤ã‚ºã¯5ã€æœ€å¤§ã‚µã‚¤ã‚ºã¯50ã«åˆ¶é™ï¼‰
                        
                        #### è§£é‡ˆã®ãƒã‚¤ãƒ³ãƒˆ
                        - **å³ä¸Š**: ãƒãƒ£ãƒ³ã‚¯æ•°ãŒå¤šãã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚‚å¤§ãã„ï¼ˆæƒ…å ±é‡ã¯å¤šã„ãŒã€ç²¾åº¦ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ï¼‰
                        - **å·¦ä¸‹**: ãƒãƒ£ãƒ³ã‚¯æ•°ãŒå°‘ãªãã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚‚å°ã•ã„ï¼ˆç²¾åº¦ã¯é«˜ã„ãŒã€æƒ…å ±ãŒä¸è¶³ã™ã‚‹å¯èƒ½æ€§ï¼‰
                        - **ãƒãƒ–ãƒ«ã®è‰²ã¨ã‚µã‚¤ã‚º**: é’ãã¦å¤§ãã„ãƒãƒ–ãƒ«ã»ã©ã€åŠ¹ç‡çš„ãªãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã§ã™
                        
                        **æ³¨**: ãƒã‚¦ã‚¹ã‚ªãƒ¼ãƒãƒ¼ã§å„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã®è©³ç´°ãªæ•°å€¤ã‚’ç¢ºèªã§ãã¾ã™
                        """)
                    
                    fig_bubble = px.scatter(
                        data_frame=plot_data,
                        x="num_chunks",
                        y="avg_chunk_len",
                        size="bubble_size",
                        color="overall_score",
                        hover_data={
                            "chunk_size": True,
                            "chunk_strategy": True,
                            "formatted_strategy": False,  # ãƒ›ãƒãƒ¼ã«ã¯è¡¨ç¤ºã—ãªã„ãŒã€ã‚«ã‚¹ã‚¿ãƒ ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§ä½¿ç”¨
                            "num_chunks": True,
                            "avg_chunk_len": ":.1f",
                            "overall_score": ".3f"
                        },
                        hover_name="formatted_strategy",  # ãƒ›ãƒãƒ¼ã«è¡¨ç¤ºã•ã‚Œã‚‹æˆ¦ç•¥åã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã®ã‚‚ã®ã«
                        labels={
                            "num_chunks": "ãƒãƒ£ãƒ³ã‚¯æ•°",
                            "avg_chunk_len": "å¹³å‡ãƒãƒ£ãƒ³ã‚¯é•·",
                            "overall_score": "ç·åˆã‚¹ã‚³ã‚¢"
                        },
                        color_continuous_scale=px.colors.sequential.Viridis,
                        width=1200,  # ã‚°ãƒ©ãƒ•ã®å¹…ã‚’æ‹¡å¤§
                        height=800,  # ã‚°ãƒ©ãƒ•ã®é«˜ã•ã‚’æ‹¡å¤§
                        color_continuous_midpoint=0.5,
                    )
                    
                    # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ›´æ–°
                    fig_bubble.update_traces(
                        textposition='middle center',
                        textfont=dict(size=12, color='white', family='Arial'),
                        marker=dict(line=dict(width=1, color='DarkSlateGrey'), opacity=0.8),
                        hovertemplate=
                        '<b>%{hovertext}</b><br>' +
                        'ãƒãƒ£ãƒ³ã‚¯æ•°: %{x}<br>' +
                        'å¹³å‡ã‚µã‚¤ã‚º: %{y}æ–‡å­—<br>' +
                        'ã‚¹ã‚³ã‚¢: %{marker.color:.2f}<extra></extra>',
                    )
                    
                    fig_bubble.update_layout(
                        title={
                            'text': f"{model_name} - ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                            'x': 0.5,
                            'xanchor': 'center'
                        },
                        coloraxis_colorbar=dict(title="ã‚¹ã‚³ã‚¢"),
                        font=dict(size=14),
                        height=500,
                        margin=dict(l=40, r=40, t=80, b=40)
                    )
                    
                    st.plotly_chart(fig_bubble, use_container_width=True)
                    st.markdown('<br>', unsafe_allow_html=True)
            
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
            for model_name, model_data in model_groups:
                if not model_data.empty and 'chunk_strategy' in model_data.columns and 'overall_score' in model_data.columns:
                    # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã”ã¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é›†è¨ˆï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¯1ã¤ã«ã¾ã¨ã‚ã‚‹ï¼‰
                    def format_strategy(row):
                        strategy = str(row['chunk_strategy']).strip()
                        # åŸºæœ¬æˆ¦ç•¥ã‚’æŠ½å‡º
                        base_strategy = strategy.split('-')[0].lower()
                        # ã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã®å ´åˆã¯åŸºæœ¬æˆ¦ç•¥åã®ã¿ã‚’è¿”ã™
                        if base_strategy in ['semantic', 'sentence', 'paragraph']:
                            return base_strategy
                        # ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æˆ¦ç•¥ã®å ´åˆã¯chunk_strategyã‚’ãã®ã¾ã¾ä½¿ç”¨
                        return strategy
                    
                    # æ­£è¦åŒ–ã—ãŸæˆ¦ç•¥åã‚’è¿½åŠ 
                    model_data['formatted_strategy'] = model_data.apply(format_strategy, axis=1)
                    
                    # ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    strategy_scores = model_data.groupby('formatted_strategy')['overall_score'].mean().sort_values(ascending=False)
                    
                    # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã”ã¨ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                    st.subheader(f"ãƒ¢ãƒ‡ãƒ«: {model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ã‚¹ã‚³ã‚¢")
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ è¡¨ç¤ºç”¨ã«æ•´å½¢
                    display_df = strategy_scores.reset_index()
                    display_df.columns = ['ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥', 'å¹³å‡ã‚¹ã‚³ã‚¢']
                    display_df['å¹³å‡ã‚¹ã‚³ã‚¢'] = display_df['å¹³å‡ã‚¹ã‚³ã‚¢'].round(3)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¡¨ç¤º
                    st.dataframe(display_df, use_container_width=True)
                    
                    # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®èª¬æ˜ã‚’è¡¨ç¤º
                    with st.expander(f"{model_name} - ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®è¦‹æ–¹", expanded=False):
                        st.markdown(f"""
                        ### ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ã‚¹ã‚³ã‚¢è¨ˆç®—æ–¹æ³•
                        - å„ãƒãƒ¼ã¯ç•°ãªã‚‹ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¡¨ã—ã¾ã™
                        - ã‚¹ã‚³ã‚¢ã¯0ã€œ1ã®ç¯„å›²ã§ã€1ã«è¿‘ã„ã»ã©è‰¯ã„çµæœã§ã™
                        - è¨ˆç®—æ–¹æ³•ï¼š
                          ```
                          å¹³å‡ã‚¹ã‚³ã‚¢ = Î£(å„è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ç·åˆã‚¹ã‚³ã‚¢) / è©•ä¾¡ãƒ‡ãƒ¼ã‚¿æ•°
                          ```
                          - ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—å¼:
                            ```
                            ç·åˆã‚¹ã‚³ã‚¢ = (faithfulness + answer_relevancy + context_recall + context_precision + answer_correctness) / 5
                            ```
                          - å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ä»¥ä¸‹ã®5ã¤ã®æŒ‡æ¨™ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ï¼š
                            - ä¿¡é ¼æ€§ (Faithfulness)
                            - å›ç­”ã®é–¢é€£æ€§ (Answer Relevancy)
                            - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§ (Context Recall)
                            - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§ (Context Precision)
                            - å›ç­”ã®æ­£ç¢ºæ€§ (Answer Correctness)
                          
                          - åŒã˜ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥å†…ã§ãƒ¢ãƒ‡ãƒ«é–“æ¯”è¼ƒãŒå¯èƒ½ã§ã™
                        - ãƒãƒ¼ã®è‰²ã¯ã‚¹ã‚³ã‚¢ã®é«˜ã•ã‚’è¡¨ã—ã€é’ã«è¿‘ã„ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã§ã™
                        - ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹æ•°å€¤ãŒå¹³å‡ã‚¹ã‚³ã‚¢ã§ã™
                        """)
                    
                    # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
                    bar_data = pd.DataFrame({
                        'strategy': strategy_scores.index,
                        'score': strategy_scores.values,
                        'score_text': [f"{x:.3f}" for x in strategy_scores.values]
                    })
                    
                    fig_bar = px.bar(
                        data_frame=bar_data,
                        x='score',
                        y='strategy',
                        orientation='h',
                        text='score_text',
                        title=f"{model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                        labels={'score': 'å¹³å‡ã‚¹ã‚³ã‚¢', 'strategy': 'ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥'},
                        color='score',
                        color_continuous_scale=px.colors.sequential.Viridis,
                        width=1200,  # ã‚°ãƒ©ãƒ•ã®å¹…ã‚’æ‹¡å¤§
                        height=800,  # ã‚°ãƒ©ãƒ•ã®é«˜ã•ã‚’æ‹¡å¤§
                    )
                    
                    # ãƒãƒ¼ã®ä¸Šã«ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                    fig_bar.update_traces(
                        texttemplate='%{x:.3f}',
                        textposition='outside',
                        hovertemplate='<b>%{y}</b><br>ã‚¹ã‚³ã‚¢: %{x:.3f}<extra></extra>',
                    )
                    
                    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                    fig_bar.update_layout(
                        title={
                            'text': f"{model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                            'x': 0.5,
                            'xanchor': 'center',
                            'font': {'size': 18}
                        },
                        xaxis=dict(range=[0, 1.1]),
                        coloraxis_showscale=False,
                        height=400,
                        margin=dict(l=100, r=40, t=100, b=40),
                        yaxis=dict(autorange="reversed"),
                        font=dict(size=14)
                    )
                    
                    # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
                    st.plotly_chart(fig_bar, use_container_width=True)
                    st.markdown('<br>', unsafe_allow_html=True)
            
            # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã”ã¨ã«ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
            if 'chunk_strategy' in results_df.columns and 'label' in results_df.columns:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’å®šç¾©
                metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision", "answer_correctness"]
                metrics_jp = ["ä¿¡é ¼æ€§", "å›ç­”ã®é–¢é€£æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§", "å›ç­”ã®æ­£ç¢ºæ€§"]
                
                # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®èª¬æ˜ã‚’è¡¨ç¤º
                with st.expander("ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®è¦‹æ–¹", expanded=False):
                    st.markdown("""
                    ### ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®è¦‹æ–¹
                    - å„è»¸ã¯ç•°ãªã‚‹è©•ä¾¡æŒ‡æ¨™ã‚’è¡¨ã—ã¦ã„ã¾ã™
                    - ã‚¹ã‚³ã‚¢ã¯0ã€œ1ã®ç¯„å›²ã§ã€1ã«è¿‘ã„ã»ã©è‰¯ã„çµæœã§ã™
                    - å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨ˆç®—ã•ã‚Œã¾ã™ï¼š
                      ```
                      å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡ = Î£(å„è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚³ã‚¢) / è©•ä¾¡ãƒ‡ãƒ¼ã‚¿æ•°
                      ```
                      - å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯ä»¥ä¸‹ã®5ã¤ã®æŒ‡æ¨™ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ï¼š
                        - ä¿¡é ¼æ€§ (Faithfulness)
                        - å›ç­”ã®é–¢é€£æ€§ (Answer Relevancy)
                        - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§ (Context Recall)
                        - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§ (Context Precision)
                        - å›ç­”ã®æ­£ç¢ºæ€§ (Answer Correctness)
                      
                      - åŒã˜ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥å†…ã§ãƒ¢ãƒ‡ãƒ«é–“æ¯”è¼ƒãŒå¯èƒ½ã§ã™
                    - ãƒã‚¦ã‚¹ã‚ªãƒ¼ãƒãƒ¼ã§è©³ç´°ãªæ•°å€¤ã‚’ç¢ºèªã§ãã¾ã™
                    """)
                
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ©ãƒ™ãƒ«ã§ãƒ«ãƒ¼ãƒ—
                for label in results_df['label'].unique():
                    # ãƒ©ãƒ™ãƒ«ã«å¯¾å¿œã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    strategy_data = results_df[results_df['label'] == label]
                    
                    if not strategy_data.empty:
                        st.subheader(f"{label} - è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒ")
                        fig_radar = go.Figure()
                        
                        # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                        for model_name in strategy_data['embedding_model'].unique():
                            # ãƒ¢ãƒ‡ãƒ«åã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            model_data = strategy_data[strategy_data['embedding_model'] == model_name]
                            
                            if not model_data.empty:
                                # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡å€¤ã‚’è¨ˆç®—
                                r_values = [model_data[m].mean() if m in model_data.columns else 0.5 for m in metrics]
                                
                                fig_radar.add_trace(go.Scatterpolar(
                                    r=r_values,
                                    theta=metrics_jp,
                                    fill='toself',
                                    name=model_name,
                                    hovertemplate='%{theta}: %{r:.2f}<extra></extra>',
                                    line=dict(width=2)
                                ))
                        
                        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                        fig_radar.update_layout(
                            polar=dict(
                                radialaxis=dict(
                                    visible=True,
                                    range=[0, 1],
                                    tickfont=dict(size=10),
                                    tickangle=0,
                                    tickformat='.1f',
                                    gridwidth=1
                                ),
                                angularaxis=dict(
                                    rotation=90,
                                    direction='clockwise',
                                    tickfont=dict(size=12),
                                    gridwidth=1
                                ),
                                bgcolor='rgba(0,0,0,0.02)'
                            ),
                            showlegend=True,
                            legend=dict(
                                orientation='h',
                                yanchor='bottom',
                                y=1.1,
                                x=0.5,
                                xanchor='center',
                                font=dict(size=10)
                            ),
                            margin=dict(l=40, r=40, t=80, b=40),
                            height=500,
                            paper_bgcolor='rgba(0,0,0,0)',
                            plot_bgcolor='rgba(0,0,0,0)'
                        )
                        
                        # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤ºï¼ˆä¸€æ„ã®keyã‚’è¿½åŠ ï¼‰
                        chart_key = f"radar_chart_{model_name}_{'_'.join(metrics_jp)}_{time.time()}"
                        st.plotly_chart(fig_radar, use_container_width=True, key=chart_key)
                        st.markdown('<br>', unsafe_allow_html=True)
            
            # çµæœã‚’DataFrameã«å¤‰æ›
        results_df = pd.DataFrame(st.session_state.bulk_evaluation_results)
        
        # ã‚°ãƒ©ãƒ•ã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
        st.markdown("---")
        st.subheader("ã‚°ãƒ©ãƒ•ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
        st.markdown("""
        <style>
            .stDownloadButton button {
                width: 100%;
                height: 3em;
                font-size: 1.2em !important;
                background-color: #4CAF50;
                color: white;
                border: none;
                border-radius: 5px;
                cursor: pointer;
                transition: background-color 0.3s;
            }
            .stDownloadButton button:hover {
                background-color: #45a049;
            }
            .stDownloadButton button:active {
                background-color: #3e8e41;
            }
        </style>
        """, unsafe_allow_html=True)
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
        if st.button("ğŸ“¥ ã™ã¹ã¦ã®ã‚°ãƒ©ãƒ•ã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", key="download_all_graphs"):
            with st.spinner("ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã—ã¦ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­..."):
                zip_data = create_zip_with_graphs(st.session_state.bulk_evaluation_results, "rag_evaluation_graphs")
                
                if zip_data:
                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨ã®ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
                    b64 = base64.b64encode(zip_data).decode()
                    href = f'<a href="data:application/zip;base64,{b64}" download="rag_evaluation_graphs.zip" class="download-link">ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
                    st.markdown(href, unsafe_allow_html=True)
                    st.success("ã‚°ãƒ©ãƒ•ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚ä¸Šè¨˜ã®ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                else:
                    st.error("ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")


# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¿ãƒ–
with tab_chatbot:
    st.header("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å¿œç­”ã‚’ç”Ÿæˆ
        response_text = ""
        # llm_modelã¯å¸¸ã«st.session_state["chat_model"]ã‚’åˆ©ç”¨
        chat_model = st.session_state.get("chat_model")
        if not chat_model:
            response_text = "ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ãŒæœªé¸æŠã§ã™ã€‚è¨­å®šã‚¿ãƒ–ã§ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
        else:
            # --- RAGãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã‚’å‘¼ã³å‡ºã—ã¦å®Ÿéš›ã®å¿œç­”ã‚’å–å¾— ---
            try:
                import requests
                BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8000")
                query_payload = {
                    "query": prompt,
                    "llm_model": chat_model,
                    "embedding_model": st.session_state.get("embedding_model", "huggingface_bge_small")
                }
                response = requests.post(f"{BACKEND_URL}/query/", json=query_payload, timeout=120)
                if response.status_code == 200:
                    data = response.json()
                    response_text = data.get("answer", "ï¼ˆå¿œç­”ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰")
                else:
                    response_text = f"APIã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}"
            except Exception as e:
                response_text = f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
        # --- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã®å¿œç­”ã®ã¿ã‚’è¡¨ç¤ºãƒ»å±¥æ­´è¿½åŠ  ---
        with st.chat_message("assistant"):
            st.markdown(response_text)
            st.session_state.chat_messages.append({"role": "assistant", "content": response_text})

        
        # ç”»é¢ã‚’æ›´æ–°
        st.rerun()

# å’è«–å‘ã‘åˆ†æã‚¿ãƒ–
with tab_thesis:
    st.header("å’è«–å‘ã‘åˆ†æ")
    
    # è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°èª¬æ˜
    with st.expander("è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°èª¬æ˜", expanded=True):
        st.markdown("""
        ### è©•ä¾¡æŒ‡æ¨™ã®è©³ç´°èª¬æ˜
        
        1. **Faithfulness (ä¿¡é ¼æ€§)**
        - å®šç¾©: ç”Ÿæˆã•ã‚ŒãŸå›ç­”ãŒã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿ å®Ÿã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’æ¸¬å®š
        - è¨ˆç®—æ–¹æ³•: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®æƒ…å ±ã®æ­£ç¢ºãªå¼•ç”¨åº¦ã‚’è©•ä¾¡
        - é‡è¦æ€§: ä¿¡é ¼æ€§ã®é«˜ã„å›ç­”ã®ç”Ÿæˆã‚’ä¿è¨¼
        
        2. **Answer Relevancy (å›ç­”ã®é–¢é€£æ€§)**
        - å®šç¾©: å›ç­”ãŒè³ªå•ã¨ã©ã‚Œã ã‘é–¢é€£ã—ã¦ã„ã‚‹ã‹ã‚’æ¸¬å®š
        - è¨ˆç®—æ–¹æ³•: è³ªå•ã¨å›ç­”ã®æ„å‘³çš„é¡ä¼¼åº¦ã‚’è©•ä¾¡
        - é‡è¦æ€§: è³ªå•ã«å¯¾ã™ã‚‹é©åˆ‡ãªå¿œç­”ã‚’ç¢ºä¿
        
        3. **Context Recall (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§)**
        - å®šç¾©: é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®æƒ…å ±ãŒæ¤œç´¢çµæœã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®š
        - è¨ˆç®—æ–¹æ³•: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®é‡è¦ãªæƒ…å ±ã®ã‚«ãƒãƒ¼ç‡ã‚’è©•ä¾¡
        - é‡è¦æ€§: å…¨é¢çš„ãªæƒ…å ±æä¾›ã‚’ä¿è¨¼
        
        4. **Context Precision (ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§)**
        - å®šç¾©: æ¤œç´¢çµæœã®ã†ã¡ã€é–¢é€£ã™ã‚‹æƒ…å ±ãŒã©ã‚Œã ã‘æ­£ç¢ºã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®š
        - è¨ˆç®—æ–¹æ³•: é–¢é€£æ€§ã¨æ­£ç¢ºæ€§ã®ä¸¡æ–¹ã‚’è©•ä¾¡
        - é‡è¦æ€§: é–¢é€£æ€§ã®é«˜ã„æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›
        
        5. **Answer Correctness (å›ç­”ã®æ­£ç¢ºæ€§)**
        - å®šç¾©: å›ç­”ã®äº‹å®Ÿé–¢ä¿‚ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’æ¸¬å®š
        - è¨ˆç®—æ–¹æ³•: å›ç­”ã®äº‹å®Ÿé–¢ä¿‚ã®æ­£ç¢ºæ€§ã‚’è©•ä¾¡
        - é‡è¦æ€§: æ­£ç¢ºãªæƒ…å ±ã®æä¾›ã‚’ä¿è¨¼
        
        6. **Overall Score (ç·åˆã‚¹ã‚³ã‚¢)**
        - å®šç¾©: ä¸Šè¨˜5ã¤ã®æŒ‡æ¨™ã®å¹³å‡å€¤
        - è¨ˆç®—æ–¹æ³•: 
          ```
          ç·åˆã‚¹ã‚³ã‚¢ = (faithfulness + answer_relevancy + context_recall + context_precision + answer_correctness) / 5
          ```
        - é‡è¦æ€§: å…¨ä½“çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©•ä¾¡
        
        **æ³¨:** ã™ã¹ã¦ã®ã‚¹ã‚³ã‚¢ã¯0ã€œ1ã®ç¯„å›²ã§æ­£è¦åŒ–ã•ã‚Œã¦ãŠã‚Šã€1ã«è¿‘ã„ã»ã©è‰¯ã„çµæœã‚’ç¤ºã—ã¾ã™ã€‚
        """)
    
    # å®Ÿé¨“è¨­å®šã®è©³ç´°èª¬æ˜
    with st.expander("å®Ÿé¨“è¨­å®šã®è©³ç´°", expanded=True):
        st.markdown("""
        ### å®Ÿé¨“è¨­å®šã®è©³ç´°èª¬æ˜
        
        1. **è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**
        - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå: è‡ªå‹•ç”ŸæˆQAã‚»ãƒƒãƒˆ
        - ãƒ‡ãƒ¼ã‚¿æ•°: è³ªå•æ•°ã«å¿œã˜ã¦å¤‰å‹•
        - ãƒ‡ãƒ¼ã‚¿æ§‹é€ : 
          - è³ªå•
          - æ­£è§£å›ç­”
          - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        - é¸å®šç†ç”±: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã«åŸºã¥ãè‡ªå‹•ç”ŸæˆQAã‚»ãƒƒãƒˆã«ã‚ˆã‚Šã€
          å®Ÿéš›ã®ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«è¿‘ã„è©•ä¾¡ãŒå¯èƒ½
        
        2. **å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**
        - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: 200ã€œ4000æ–‡å­—
        - ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: 0ã€œ1000æ–‡å­—
        - ãƒ¢ãƒ‡ãƒ«è¨­å®š: 
          - LLM: Ollama, OpenAI
          - Embedding: HuggingFace, OpenAI
        
        3. **å†ç¾æ€§è¨­å®š**
        - ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: å›ºå®šå€¤ã‚’ä½¿ç”¨
        - ç’°å¢ƒè¨­å®š: 
          - Pythonãƒãƒ¼ã‚¸ãƒ§ãƒ³
          - ä½¿ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³
          - ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç’°å¢ƒ
        - å®Ÿè¡Œæ‰‹é †: 
          1. PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
          2. ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
          3. è©•ä¾¡å®Ÿè¡Œ
          4. çµæœç¢ºèª
        """)
    
    # çµæœã®çµ±è¨ˆçš„åˆ†æ
    with st.expander("çµ±è¨ˆçš„åˆ†æçµæœ", expanded=True):
        if st.session_state.bulk_evaluation_results:
            # çµæœã‚’DataFrameã«å¤‰æ›
            results_df = pd.DataFrame(st.session_state.bulk_evaluation_results)
            
            # è©•ä¾¡æŒ‡æ¨™ã®çµ±è¨ˆå€¤ã‚’è¡¨ç¤º
            st.subheader("è©•ä¾¡æŒ‡æ¨™ã®çµ±è¨ˆå€¤")
            metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision", "answer_correctness", "overall_score"]
            stats_df = pd.DataFrame({
                "æŒ‡æ¨™": metrics,
                "å¹³å‡å€¤": results_df[metrics].mean(),
                "æ¨™æº–åå·®": results_df[metrics].std(),
                "æœ€å°å€¤": results_df[metrics].min(),
                "æœ€å¤§å€¤": results_df[metrics].max(),
                "ä¸­å¤®å€¤": results_df[metrics].median()
            })
            st.dataframe(stats_df, use_container_width=True)
            
            # ç›¸é–¢åˆ†æ
            st.subheader("æŒ‡æ¨™é–“ã®ç›¸é–¢ä¿‚æ•°")
            corr_matrix = results_df[metrics].corr()
            st.dataframe(corr_matrix, use_container_width=True)
            
            # ä¿¡é ¼åŒºé–“ã®è¨ˆç®—
            st.subheader("ä¿¡é ¼åŒºé–“")
            confidence_intervals = {
                "æŒ‡æ¨™": metrics,
                "95%ä¿¡é ¼åŒºé–“ (ä¸‹é™)": [results_df[metric].quantile(0.025) for metric in metrics],
                "95%ä¿¡é ¼åŒºé–“ (ä¸Šé™)": [results_df[metric].quantile(0.975) for metric in metrics]
            }
            st.dataframe(pd.DataFrame(confidence_intervals), use_container_width=True)
        else:
            st.info("çµ±è¨ˆçš„åˆ†æçµæœã¯ä¸€æ‹¬è©•ä¾¡å®Ÿè¡Œå¾Œã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            
            # ä¸€æ‹¬è©•ä¾¡å®Ÿè¡Œå‰ã®æº–å‚™çŠ¶æ³ç¢ºèª
            if st.session_state.get("text"):
                st.write("æº–å‚™çŠ¶æ³:")
                
                # æ–‡æ›¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çŠ¶æ³
                st.write("- æ–‡æ›¸: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿")
                
                # ãƒãƒ£ãƒ³ã‚¯è¨­å®šã®ç¢ºèª
                if all(key in st.session_state for key in ["chunk_size", "chunk_overlap", "embedding_model"]):
                    st.write(f"- ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {st.session_state.chunk_size}")
                    st.write(f"- ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {st.session_state.chunk_overlap}")
                    st.write(f"- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {st.session_state.embedding_model}")
                else:
                    st.write("- ãƒãƒ£ãƒ³ã‚¯è¨­å®š: æœªè¨­å®š")
                    
                # ãƒ¢ãƒ‡ãƒ«é¸æŠã®ç¢ºèª
                if "llm_model" in st.session_state:
                    # ãƒ¢ãƒ‡ãƒ«ãŒãƒªã‚¹ãƒˆã®å ´åˆ
                    if isinstance(st.session_state.llm_model, list):
                        models = ", ".join(st.session_state.llm_model)
                        st.write(f"- LLMãƒ¢ãƒ‡ãƒ«: {models}")
                        # MiniLMã®ç‰¹æ€§èª¬æ˜
                        if any("mini" in model.lower() for model in st.session_state.llm_model):
                            st.markdown("**æ³¨:** MiniLMãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒãƒ£ãƒ³ã‚¯åŒ–ã®å½±éŸ¿ãŒä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦å°ã•ã„ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚ã“ã‚Œã¯MiniLMã®è»½é‡ãªæ€§è³ªã«ã‚ˆã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªæ–‡æ›¸ç†è§£ãŒå¿…è¦ãªå ´åˆã¯ã€ã‚ˆã‚Šé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
                    else:
                        st.write(f"- LLMãƒ¢ãƒ‡ãƒ«: {st.session_state.llm_model}")
                        # MiniLMã®ç‰¹æ€§èª¬æ˜
                        if "mini" in st.session_state.llm_model.lower():
                            st.markdown("**æ³¨:** MiniLMãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒãƒ£ãƒ³ã‚¯åŒ–ã®å½±éŸ¿ãŒä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦å°ã•ã„ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚ã“ã‚Œã¯MiniLMã®è»½é‡ãªæ€§è³ªã«ã‚ˆã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªæ–‡æ›¸ç†è§£ãŒå¿…è¦ãªå ´åˆã¯ã€ã‚ˆã‚Šé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
                else:
                    st.write("- LLMãƒ¢ãƒ‡ãƒ«: æœªé¸æŠ")
            else:
                st.write("æº–å‚™çŠ¶æ³:")
                st.write("- æ–‡æ›¸: æœªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
                st.write("- ãƒãƒ£ãƒ³ã‚¯è¨­å®š: æœªè¨­å®š")
                st.write("- ãƒ¢ãƒ‡ãƒ«é¸æŠ: æœªé¸æŠ")
    
    # å®Ÿé¨“çµæœã®è§£é‡ˆæ”¯æ´
    with st.expander("å®Ÿé¨“çµæœã®è§£é‡ˆæ”¯æ´", expanded=True):
        st.markdown("""
        ### å®Ÿé¨“çµæœã®è§£é‡ˆæ”¯æ´
        
        1. **ã‚¹ã‚³ã‚¢ã®è§£é‡ˆ**
        - 0.8ä»¥ä¸Š: éå¸¸ã«è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        - 0.6ã€œ0.8: è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        - 0.4ã€œ0.6: å¹³å‡çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        - 0.2ã€œ0.4: ä½ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        - 0.2ä»¥ä¸‹: éå¸¸ã«ä½ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        
        2. **çµæœã®åˆ†æãƒã‚¤ãƒ³ãƒˆ**
        - å„æŒ‡æ¨™ã®ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã•
        - ãƒ¢ãƒ‡ãƒ«é–“ã®ç›¸å¯¾çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å½±éŸ¿
        - çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç¢ºèª
        
        3. **æ”¹å–„ã®ãŸã‚ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**
        - ã‚¹ã‚³ã‚¢ãŒä½ã„æŒ‡æ¨™ã®ç‰¹å®š
        - ãƒ¢ãƒ‡ãƒ«ã®é¸æŠè‚¢ã®å†è©•ä¾¡
        - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã®æœ€é©åŒ–
        - è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ‹¡å……
        """)

# ã‚¿ãƒ–5: è©•ä¾¡å±¥æ­´ã‚¿ãƒ–
with tab_history:
    try:
        from evaluation_history_ui import show_evaluation_history
        show_evaluation_history(BACKEND_URL)
    except ImportError as e:
        st.error(f"è©•ä¾¡å±¥æ­´UIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        st.info("è©•ä¾¡å±¥æ­´æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    except Exception as e:
        st.error(f"è©•ä¾¡å±¥æ­´è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
        st.info("è©•ä¾¡å±¥æ­´æ©Ÿèƒ½ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")