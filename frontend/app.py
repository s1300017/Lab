from datetime import datetime
from pytz import timezone

def jst_now_str():
    return datetime.now(timezone('Asia/Tokyo')).strftime('%Y-%m-%d %H:%M:%S JST')

import os
import json
import streamlit as st
import streamlit.components.v1 as components
import base64
import json
import pandas as pd
import plotly.express as px
import requests
from typing import List, Dict, Any, Optional, Tuple, Literal
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import plotly.graph_objects as go  # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆç­‰ã§ä½¿ç”¨
from openai import OpenAI
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

st.set_page_config(layout="wide")
st.title("RAGè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")

# --- Session State Initialization ---
def init_session_state():
    if 'text' not in st.session_state:
        st.session_state.text = ""
    if 'chunks' not in st.session_state:
        st.session_state.chunks = []
    if 'evaluation_results' not in st.session_state:
        st.session_state.evaluation_results = None
    if 'bulk_evaluation_results' not in st.session_state:
        st.session_state.bulk_evaluation_results = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'llm_model' not in st.session_state:
        st.session_state.llm_model = "ollama_llama2" # Default to Ollama
    if 'embedding_model' not in st.session_state:
        st.session_state.embedding_model = "huggingface_bge_small" # Default to HuggingFace

init_session_state()

# --- Backend API Calls ---
# --- ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã®URLè¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã¯localhostã‚’æ¨å¥¨ï¼‰ ---
# secrets.tomlãŒå­˜åœ¨ã—ãªãã¦ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„ã‚ˆã†ä¾‹å¤–å‡¦ç†ã‚’è¿½åŠ 
# Dockerç’°å¢ƒã§ã¯backendã‚µãƒ¼ãƒ“ã‚¹åã§APIã«æ¥ç¶šã™ã‚‹ã®ãŒæ¨å¥¨
import os
try:
    BACKEND_URL = st.secrets.get('BACKEND_URL', os.environ.get('BACKEND_URL', 'http://backend:8000'))  # Dockeræ™‚ã¯backend:8000ã€ãƒ­ãƒ¼ã‚«ãƒ«æ™‚ã¯ç’°å¢ƒå¤‰æ•°orlocalhost
except Exception as e:
    print(f"[WARNING] st.secretsèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
    BACKEND_URL = os.environ.get('BACKEND_URL', 'http://backend:8000')


def clear_database():
    try:
        response = requests.post(f"{BACKEND_URL}/clear_db/")
        if response.status_code == 200:
            st.success("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ­£å¸¸ã«ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼")
            # Clear session state related to old data
            st.session_state.text = ""
            st.session_state.chunks = []
            st.session_state.evaluation_results = None
            st.session_state.chat_history = []
        else:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.text}")
    except requests.exceptions.RequestException as e:
        st.error(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")

# --- localStorageãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
def save_state_to_localstorage():
    state_keys = [
        "file_id", "text", "qa_questions", "qa_answers", "uploaded_file_name", "uploaded_file_bytes",
        "evaluation_results", "bulk_evaluation_results", "chunks", "chat_history",
        "current_evaluation", "evaluation_history", "active_tab",
        "tab1_content", "tab2_content", "tab3_content", "tab4_content"
    ]
    state = {}
    for k in state_keys:
        v = st.session_state.get(k)
        if v is not None:
            # ãƒã‚¤ãƒˆåˆ—ã¯base64ã§ä¿å­˜
            if k == "uploaded_file_bytes" and isinstance(v, bytes):
                state[k] = base64.b64encode(v).decode('utf-8')
            # è©•ä¾¡å±¥æ­´ã¨ã‚¿ãƒ–å†…å®¹ã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
            elif k in ["evaluation_history", "bulk_evaluation_results", "tab1_content", "tab2_content", "tab3_content", "tab4_content"]:
                state[k] = json.dumps(v)
            else:
                state[k] = v
    # JSONæ–‡å­—åˆ—ã‚’UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    js = json.dumps(state, ensure_ascii=False)
    encoded_js = base64.b64encode(js.encode('utf-8')).decode('utf-8')
    components.html(f"""
    <script>
    localStorage.setItem('rag_app_state', '{encoded_js}');
    window.parent.postMessage({{streamlitMessage: 'localStorageSaved'}}, '*');
    </script>
    """, height=0)

# --- session_stateã®åˆæœŸåŒ– ---
def init_session_state():
    default_state = {
        "file_id": None,
        "text": "",
        "qa_questions": [],
        "qa_answers": [],
        "uploaded_file_name": "",
        "uploaded_file_bytes": None,
        "evaluation_results": {},  # è©•ä¾¡çµæœ
        "bulk_evaluation_results": {},  # ãƒãƒ«ã‚¯è©•ä¾¡çµæœ
        "chunks": [],  # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿
        "chat_history": [],  # ãƒãƒ£ãƒƒãƒˆå±¥æ­´
        "current_evaluation": None,  # ç¾åœ¨ã®è©•ä¾¡ã‚»ãƒƒã‚·ãƒ§ãƒ³
        "evaluation_history": [],  # è©•ä¾¡å±¥æ­´
        "active_tab": "tab1",  # ç¾åœ¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ãƒ–
        "tab1_content": {"chat_history": []},  # ã‚¿ãƒ–1ã®è¡¨ç¤ºå†…å®¹
        "tab2_content": {},  # ã‚¿ãƒ–2ã®è¡¨ç¤ºå†…å®¹
        "tab3_content": {},  # ã‚¿ãƒ–3ã®è¡¨ç¤ºå†…å®¹
        "tab4_content": {},  # ã‚¿ãƒ–4ã®è¡¨ç¤ºå†…å®¹
        "_localstorage_loaded": False
    }
    for k, v in default_state.items():
        if k not in st.session_state:
            st.session_state[k] = v

# --- UI Layout ---
with st.sidebar:
    # --- ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³ ---
    # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚„ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç¤º
    from streamlit_js_eval import streamlit_js_eval
    local_state = streamlit_js_eval(js_expressions="localStorage.getItem('rag_app_state')", key="check_localstorage")
    has_data = local_state is not None or any(
        st.session_state.get(k) not in [None, "", [], {}]
        for k in ["text", "qa_questions", "qa_answers", "uploaded_file_name", "uploaded_file_bytes"]
    )
    
    if has_data:
        if st.button("ãƒªã‚»ãƒƒãƒˆï¼ˆã™ã¹ã¦ã‚¯ãƒªã‚¢ï¼‰"):
            # 1. localStorageã‚’ã‚¯ãƒªã‚¢
            components.html("""
            <script>
            localStorage.removeItem('rag_app_state');
            window.parent.postMessage({streamlitMessage: 'localStorageCleared'}, '*');
            </script>
            """, height=0)
            
            # 2. session_stateã‚’åˆæœŸåŒ–
            init_session_state()
            
            # 3. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢
            try:
                response = requests.post(f"{BACKEND_URL}/clear_db/")
                if response.status_code == 200:
                    st.success("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ­£å¸¸ã«ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼")
                else:
                    st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.text}")
            except requests.exceptions.RequestException as e:
                st.error(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
            
            # 4. ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿
            st.rerun()
    else:
        st.warning("""
        ğŸ“ ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“
        
        PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆã§ãã¾ã™ã€‚
        """)

    # --- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ãƒœã‚¿ãƒ³ ---
    if st.button("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã¿åˆæœŸåŒ–"):
        try:
            response = requests.post(f"{BACKEND_URL}/clear_db/")
            if response.status_code == 200:
                st.success("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ­£å¸¸ã«ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸï¼")
                st.session_state.text = ""
                st.session_state.chunks = []
                st.session_state.evaluation_results = None
                st.session_state.chat_history = []
            else:
                st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {response.text}")
        except requests.exceptions.RequestException as e:
            st.error(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¥ç¶šã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")

    st.header("è¨­å®š")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’APIçµŒç”±ã§å–å¾—
    import requests
    def fetch_models():
        try:
            resp = requests.get(f"{BACKEND_URL}/list_models")
            resp.raise_for_status()
            data = resp.json()
            return data.get("models", [])
        except Exception as e:
            st.error(f"ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    models = fetch_models()
    model_options = [m['display_name'] for m in models] if models else ["ollama_llama2", "openai"]
    model_names = [m['name'] for m in models] if models else ["ollama_llama2", "openai"]

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠãƒ­ã‚¸ãƒƒã‚¯
    default_idx = 0
    if 'llm_model' in st.session_state and st.session_state.llm_model in model_names:
        default_idx = model_names.index(st.session_state.llm_model)
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    st.subheader("ãƒ¢ãƒ‡ãƒ«è¨­å®š")
    
    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã‚’ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        st.warning("è­¦å‘Š: OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    else:
        st.sidebar.success("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™")
        
    llm_model = st.selectbox(
        "LLMãƒ¢ãƒ‡ãƒ«",
        options=model_options,
        index=model_options.index(st.session_state.llm_model) if st.session_state.llm_model in model_options else 0,
        key="llm_model_select"
    )
    
    chat_model_options = ["gpt-4o-mini", "gpt-3.5-turbo", "llama3-70b-8192"]
    chat_model = st.selectbox(
        "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆãƒ¢ãƒ‡ãƒ«",
        options=chat_model_options,
        index=0,
        key="chat_model_select"
    )
    
    # Embeddingãƒ¢ãƒ‡ãƒ«ã‚‚åŒæ§˜ã«APIåŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦æ‹¡å¼µå¯ï¼‰
    embedding_options = {
        "OpenAI": "openai",
        "bge-smallï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é«˜é€Ÿï¼‰": "huggingface_bge_small",
        "MiniLMï¼ˆè»½é‡ãƒ»å¤šè¨€èªï¼‰": "huggingface_miniLM",
    }
    emb_default_idx = 0 if st.session_state.embedding_model == "huggingface_bge_small" else 1
    st.session_state.embedding_model = st.selectbox(
        "Embeddingãƒ¢ãƒ‡ãƒ«",
        embedding_options,
        index=emb_default_idx
    )

    import io
    # --- file_idãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ã‚ã‚Œã°çŠ¶æ…‹ã‚’å¾©å…ƒ ---
    # --- localStorageã‹ã‚‰å¾©å…ƒ ---
    def load_state_from_localstorage():
        from streamlit_js_eval import streamlit_js_eval
        local_state = streamlit_js_eval(js_expressions="localStorage.getItem('rag_app_state')", key="load_localstorage")
        if local_state and not st.session_state.get("_localstorage_loaded", False):
            try:
                # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèª
                if isinstance(local_state, str):
                    local_state = local_state.encode('utf-8')
                # base64ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦JSONæ–‡å­—åˆ—ã‚’å¾©å…ƒ
                js_bytes = base64.b64decode(local_state)
                js = js_bytes.decode("utf-8")
                state = json.loads(js)
                for k, v in state.items():
                    if k == "uploaded_file_bytes":
                        st.session_state[k] = base64.b64decode(v)
                    elif k in ["evaluation_history", "bulk_evaluation_results", "tab1_content", "tab2_content", "tab3_content", "tab4_content"]:
                        # è©•ä¾¡å±¥æ­´ã¨ã‚¿ãƒ–å†…å®¹ã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ãƒ‡ã‚³ãƒ¼ãƒ‰
                        st.session_state[k] = json.loads(v)
                    else:
                        st.session_state[k] = v
                st.session_state["_localstorage_loaded"] = True
            except Exception as e:
                st.warning(f"localStorageå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
                init_session_state()    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã§ã‚‚session_stateã‚’ã‚¯ãƒªã‚¢
                init_session_state()
        # localStorageå–å¾—æ™‚ã¯window.postMessageã§å€¤ãŒè¿”ã‚‹ãŒã€Streamlitæ¨™æº–ã§ã¯ç›´æ¥å—ã‘å–ã‚Œãªã„ãŸã‚ã€
        # ã“ã“ã§ã¯ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚„å¾©å…ƒã®ãŸã³ã«save_state_to_localstorage()ã€ã‚’å‘¼ã¶ã“ã¨ã§æ°¸ç¶šåŒ–ã™ã‚‹

    load_state_from_localstorage()

    if "file_id" in st.session_state and not st.session_state.get("text"):
        try:
            resp = requests.get(f"{BACKEND_URL}/get_extracted/{st.session_state['file_id']}")
            if resp.status_code == 200:
                data = resp.json()
                st.session_state["text"] = data.get("text", "")
                st.session_state["qa_questions"] = data.get("questions", [])
                st.session_state["qa_answers"] = data.get("answers", [])
                # --- ãƒ•ã‚¡ã‚¤ãƒ«åã¨ãƒã‚¤ãƒˆåˆ—ã‚‚å¿…ãšå¾©å…ƒ ---
                st.session_state["uploaded_file_name"] = data.get("file_name", f"{st.session_state['file_id']}.pdf")
                if "pdf_bytes_base64" in data:
                    st.session_state["uploaded_file_bytes"] = base64.b64decode(data["pdf_bytes_base64"])
                st.success(f"å‰å›ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ID: {st.session_state['file_id']} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒã—ã¾ã—ãŸ")
                save_state_to_localstorage()
            else:
                st.warning("ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒã«å¤±æ•—ã—ã¾ã—ãŸã€‚file_idã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚")
                del st.session_state["file_id"]
        except Exception as e:
            st.warning(f"ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒã«å¤±æ•—: {e}")
            del st.session_state["file_id"]
    # ã™ã§ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ã©ã†ã‹ã§UIã‚’åˆ†å²
    if "uploaded_file_bytes" in st.session_state and "uploaded_file_name" in st.session_state:
        st.info(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆ: {st.session_state['uploaded_file_name']}")
        # å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã„å ´åˆã®ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
        if st.button("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚„ã‚Šç›´ã™"):
            for key in ["uploaded_file_bytes", "uploaded_file_name", "uploaded_file_size", "text", "qa_questions", "qa_answers", "file_id"]:
                if key in st.session_state:
                    del st.session_state[key]
            components.html("""
            <script>localStorage.removeItem('rag_app_state');</script>
            """, height=0)
            st.rerun()
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’BytesIOã§å¾©å…ƒ
        uploaded_file = io.BytesIO(st.session_state["uploaded_file_bytes"])
        uploaded_file.name = st.session_state["uploaded_file_name"]
        # ã¾ã ãƒ†ã‚­ã‚¹ãƒˆã‚„QAãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ç„¡ã‘ã‚Œã°PDFå‡¦ç†ã‚’å®Ÿè¡Œ
        if not st.session_state.get("text"):
            with st.spinner('PDFã‚’å‡¦ç†ä¸­...'):
                files = {'file': (uploaded_file.name, uploaded_file, 'application/pdf')}
                try:
                    response = requests.post(f"{BACKEND_URL}/uploadfile/", files=files)
                    if response.status_code == 200:
                        data = response.json()
                        if "file_id" in data:
                            st.session_state["file_id"] = data["file_id"]
                        if 'questions' in data and 'answers' in data:
                            st.session_state.text = data['text']
                            st.session_state.qa_questions = data['questions']
                            st.session_state.qa_answers = data['answers']
                            st.success("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ»è³ªå•ãƒ»å›ç­”ã‚»ãƒƒãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã—ãŸã€‚ä»¥é™ã®è©•ä¾¡å‡¦ç†ã§ã“ã®ã‚»ãƒƒãƒˆãŒä½¿ã‚ã‚Œã¾ã™ã€‚")
                            st.write("### è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè³ªå•:")
                            for i, q in enumerate(data['questions']):
                                st.write(f"Q{i+1}: {q}")
                            st.write("### è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”:")
                            for i, a in enumerate(data['answers']):
                                st.write(f"A{i+1}: {a}")
                        else:
                            st.error(f"PDFå‡¦ç†APIã®è¿”å´å†…å®¹ã«questions/answersãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“: {data}")
                        save_state_to_localstorage()
                    else:
                        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {response.text}")
                except requests.exceptions.RequestException as e:
                    st.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        else:
            # æ—¢ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ»QAãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºã®ã¿
            st.success("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ»è³ªå•ãƒ»å›ç­”ã‚»ãƒƒãƒˆã¯æ—¢ã«æŠ½å‡ºæ¸ˆã¿ã§ã™ã€‚")
            st.write("### è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè³ªå•:")
            for i, q in enumerate(st.session_state.qa_questions):
                st.write(f"Q{i+1}: {q}")
            st.write("### è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”:")
            for i, a in enumerate(st.session_state.qa_answers):
                st.write(f"A{i+1}: {a}")
        save_state_to_localstorage()
    else:
        # ã¾ã ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯file_uploaderã‚’è¡¨ç¤º
        uploaded_file = st.file_uploader("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
        if uploaded_file is not None:
            st.session_state["uploaded_file_bytes"] = uploaded_file.getvalue()
            st.session_state["uploaded_file_name"] = uploaded_file.name
            st.session_state["uploaded_file_size"] = uploaded_file.size
            save_state_to_localstorage()
            st.rerun()

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¿ãƒ–å®šç¾©
tab1, tab2, tab3, tab4, tab_chatbot = st.tabs(["ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š", "è©•ä¾¡", "ä¸€æ‹¬è©•ä¾¡", "æ¯”è¼ƒ", "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ"])

# ã‚¿ãƒ–1: ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š
with tab1:
    if st.session_state.text:
        st.subheader("ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°è¨­å®š")
        chunk_method = st.radio("ãƒãƒ£ãƒ³ã‚¯åŒ–æ–¹å¼", ["recursive", "semantic"], index=0, help="recursive: æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹, semantic: æ„å‘³ãƒ™ãƒ¼ã‚¹")
        chunk_size = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", 200, 4000, 1000, 100)
        chunk_overlap = st.slider("ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—", 50, 1000, 200, 50)
        embedding_model = st.selectbox("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ« (semanticæ™‚å¿…é ˆ)", ["huggingface_bge_small", "openai"], index=0)
        if st.button("ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–"):
            with st.spinner('ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¨ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œä¸­...'):
                # 1. Chunk
                payload = {
                    "text": st.session_state.text,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "chunk_method": chunk_method,
                }
                if chunk_method == "semantic":
                    payload["embedding_model"] = embedding_model

                chunk_response = requests.post(f"{BACKEND_URL}/chunk/", json=payload)
                if chunk_response.status_code == 200:
                    st.session_state.chunks = chunk_response.json()['chunks']
                    # 2. Embed
                    embed_payload = {"chunks": st.session_state.chunks, "embedding_model": st.session_state.embedding_model}
                    embed_response = requests.post(f"{BACKEND_URL}/embed_and_store/", json=embed_payload)
                    if embed_response.status_code == 200:
                        st.success(f"{len(st.session_state.chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç”Ÿæˆã—ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã—ãŸã€‚")
                        # --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ---
                        if not st.session_state.text:
                            st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€è¨­å®šã‚’è¡Œã£ã¦ãã ã•ã„ã€‚")

# ã‚¿ãƒ–2: è©•ä¾¡
# PDFãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒãƒ£ãƒƒãƒˆç”»é¢ã®ã¿è¡¨ç¤º
if 'uploaded_file_bytes' not in st.session_state:
    st.warning("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

with tab2:
    st.header("è©•ä¾¡ã®å®Ÿè¡Œã¨çµæœ")
    
    # è©•ä¾¡å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³
    with st.expander("è©•ä¾¡ã‚’å®Ÿè¡Œ", expanded=True):
        st.subheader("è©•ä¾¡ã®å®Ÿè¡Œ")
        
        # è³ªå•ã¨å›ç­”ã®å…¥åŠ›
        questions = st.text_area("è©•ä¾¡ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ï¼ˆ1è¡Œã«1ã¤ï¼‰", 
                              value="\n".join(st.session_state.get('qa_questions', [])), 
                              height=150,
                              help="è©•ä¾¡ã—ãŸã„è³ªå•ã‚’1è¡Œãšã¤å…¥åŠ›ã—ã¦ãã ã•ã„")
        
        answers = st.text_area("å›ç­”ã‚’å…¥åŠ›ï¼ˆ1è¡Œã«1ã¤ã€è³ªå•ã¨é †ç•ªã‚’åˆã‚ã›ã¦ãã ã•ã„ï¼‰", 
                             value="\n".join(st.session_state.get('qa_answers', [])), 
                             height=150,
                             help="è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’1è¡Œãšã¤å…¥åŠ›ã—ã¦ãã ã•ã„")
        
        # è©•ä¾¡å®Ÿè¡Œãƒœã‚¿ãƒ³
        if st.button("è©•ä¾¡ã‚’å®Ÿè¡Œ", key="evaluate_button_evaluation_tab"):
            questions = [q.strip() for q in questions.split('\n') if q.strip()]
            answers = [a.strip() for a in answers.split('\n') if a.strip()]
            
            if not questions:
                st.warning("è©•ä¾¡ã™ã‚‹è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            elif len(questions) != len(answers):
                st.warning("è³ªå•ã¨å›ç­”ã®æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚")
            else:
                with st.spinner("è©•ä¾¡ã‚’å®Ÿè¡Œä¸­... ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"):
                    try:
                        evaluation_payload = {
                            "questions": questions,
                            "answers": answers,
                            "contexts": ["" for _ in questions],
                            "model": st.session_state.llm_model
                        }
                        
                        response = requests.post(
                            f"{BACKEND_URL}/evaluate/", 
                            json=evaluation_payload
                        )
                        
                        if response.status_code == 200:
                            st.session_state.evaluation_results = response.json()
                            st.session_state.qa_questions = questions
                            st.session_state.qa_answers = answers
                            st.success("è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                            st.rerun()
                        else:
                            st.error(f"è©•ä¾¡ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {response.text}")
                    except Exception as e:
                        st.error(f"è©•ä¾¡ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    # è©•ä¾¡çµæœè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.subheader("è©•ä¾¡çµæœ")
    if 'evaluation_results' in st.session_state and st.session_state.evaluation_results:
        eval_results = st.session_state.evaluation_results
        
        # è©•ä¾¡çµæœã‚’è¡¨å½¢å¼ã§è¡¨ç¤º
        st.subheader("è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
        
        # ã‚¹ã‚³ã‚¢ã®è¡¨ç¤º
        if 'scores' in eval_results:
            scores = eval_results['scores']
            st.write("### ã‚¹ã‚³ã‚¢")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ãƒ•ã‚¡ã‚¯ãƒˆæ•´åˆæ€§", f"{scores.get('faithfulness', 0):.2f}")
            with col2:
                st.metric("å›ç­”é–¢é€£æ€§", f"{scores.get('answer_relevancy', 0):.2f}")
            with col3:
                st.metric("æ–‡è„ˆå†ç¾ç‡", f"{scores.get('context_recall', 0):.2f}")
            with col4:
                st.metric("æ–‡è„ˆé©åˆç‡", f"{scores.get('context_precision', 0):.2f}")
        
        # è©³ç´°ãªè©•ä¾¡çµæœã‚’è¡¨ç¤º
        if 'results' in eval_results:
            st.write("### è³ªå•ã”ã¨ã®è©³ç´°")
            for i, result in enumerate(eval_results['results']):
                with st.expander(f"è³ªå• {i+1}: {result.get('question', '')}"):
                    st.write(f"**è³ªå•**: {result.get('question', '')}")
                    st.write(f"**å›ç­”**: {result.get('answer', '')}")
                    st.write(f"**ã‚¹ã‚³ã‚¢**: {result.get('score', 'N/A')}")
                    if 'details' in result:
                        st.json(result['details'])
    else:
        st.info("è©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®ãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

# ä¸€æ‹¬è©•ä¾¡ã‚¿ãƒ–
with tab3:
    st.header("ä¸€æ‹¬è©•ä¾¡")
    st.markdown("Embeddingãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹å¼ãƒ»ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å…¨çµ„ã¿åˆã‚ã›ã§ä¸€æ‹¬è‡ªå‹•è©•ä¾¡ã‚’è¡Œã„ã¾ã™ã€‚")

    # Embeddingãƒ¢ãƒ‡ãƒ«ã®è¤‡æ•°é¸æŠ
    embedding_options = {
        "bge-smallï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é«˜é€Ÿï¼‰": "huggingface_bge_small",
        "MiniLMï¼ˆè»½é‡ãƒ»å¤šè¨€èªï¼‰": "huggingface_miniLM",
        "OpenAI": "openai",
    }
    embedding_labels = list(embedding_options.keys())
    embedding_values = list(embedding_options.values())
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§bge-smallã‚’é¸æŠ
    selected_labels = st.multiselect(
        "Embeddingãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        embedding_labels,
        default=[embedding_labels[0]],
        key="bulk_embeddings_tab3"
    )
    selected_embeddings = [embedding_options[label] for label in selected_labels]

    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹å¼ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¤‡æ•°é¸æŠ
    chunk_methods = st.multiselect(
        "ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹å¼ã‚’é¸æŠ",
        ["fixed", "recursive", "semantic", "sentence", "paragraph"],
        default=["fixed", "recursive", "semantic"]
    )
    chunk_sizes = st.multiselect(
        "ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰",
        [128, 256, 500, 1000, 1500, 2000],
        default=[500, 1000]
    )
    chunk_overlaps = st.multiselect(
        "ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰",
        [0, 32, 64, 100, 200, 300],
        default=[0, 100, 200]
    )
    st.caption("â€»Embeddingãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²æ–¹å¼ãƒ»ã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®å…¨çµ„ã¿åˆã‚ã›ã§è‡ªå‹•ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™")

    if st.button("ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œ", key="bulk_evaluate_button_2"):
        # ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not st.session_state.get("text"):
            st.error("è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ã¾ãšãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.stop()
            
        with st.spinner("ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œä¸­..."):
            import concurrent.futures
            from concurrent.futures import ThreadPoolExecutor
            import time
            
            bulk_results = []
            invalid_combinations = []
            valid_combinations = []
            
            # æœ‰åŠ¹ãªçµ„ã¿åˆã‚ã›ã®ã¿æŠ½å‡º
            for method in chunk_methods:
                if method in ["fixed", "recursive", "semantic"]:
                    for size in chunk_sizes:
                        for overlap in chunk_overlaps:
                            if size > overlap:
                                valid_combinations.append((method, size, overlap))
                            else:
                                invalid_combinations.append((method, size, overlap))
                else:
                    # size/overlapã‚’ä½¿ã‚ãªã„æ–¹å¼ã¯ä¸€åº¦ã ã‘Noneã§è¿½åŠ 
                    valid_combinations.append((method, None, None))
            
            if not valid_combinations:
                st.error("æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã®çµ„ã¿åˆã‚ã›ãŒã‚ã‚Šã¾ã›ã‚“ã€‚chunk_size > overlap ã¨ãªã‚‹ã‚ˆã†ã«é¸æŠã—ã¦ãã ã•ã„ã€‚")
                st.stop()
            
            if invalid_combinations:
                st.warning(f"chunk_size <= overlap ã¨ãªã‚‹ä¸æ­£ãªçµ„ã¿åˆã‚ã›ã¯è‡ªå‹•çš„ã«é™¤å¤–ã—ã¾ã—ãŸ: {invalid_combinations}")
            
            # é€²æ—ãƒãƒ¼ã€ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºã®è¨­å®š
            progress_bar = st.progress(0)
            progress_text = st.empty()
            status_display = st.empty()
            total_tasks = len(selected_embeddings) * len(valid_combinations)
            
            # å®Œäº†ã‚¿ã‚¹ã‚¯æ•°ã‚’è¿½è·¡ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆï¼ˆãƒŸãƒ¥ãƒ¼ã‚¿ãƒ–ãƒ«ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰
            completed_tasks = [0]
            
            # è©•ä¾¡ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
            def evaluate_single(emb, method, size, overlap):
                try:
                    # ãƒ†ã‚­ã‚¹ãƒˆã®å­˜åœ¨ã‚’å†ç¢ºèª
                    text = st.session_state.get("text")
                    qa_questions = st.session_state.get("qa_questions", [])
                    qa_answers = st.session_state.get("qa_answers", [])
                    
                    if not text:
                        st.error("è©•ä¾¡å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                        return None
                        
                    if not qa_questions or not qa_answers:
                        st.error("è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€Q&Aã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                        return None
                        
                    payload = {
                        "embedding_model": emb,
                        "chunk_methods": [method],
                        "chunk_sizes": [size] if size is not None else [1000],
                        "chunk_overlaps": [overlap] if overlap is not None else [200],
                        "text": text,
                        "questions": qa_questions,
                        "answers": qa_answers,
                    }
                    
                    response = requests.post(
                        f"{BACKEND_URL}/bulk_evaluate/", 
                        json=payload, 
                        timeout=300
                    )
                    
                    completed_tasks[0] += 1
                    progress_bar.progress(min(completed_tasks[0] / total_tasks, 1.0))
                    progress_text.text(f"å®Œäº†: {completed_tasks[0]} / {total_tasks} ä»¶")
                    
                    if response.status_code == 200:
                        return response.json()
                    else:
                        st.error("è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°:")
                        st.json({
                            "status_code": response.status_code,
                            "error": response.text,
                            "request_payload": payload
                        })
                        return None
                        
                except requests.exceptions.RequestException as e:
                    st.error(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return None
                except json.JSONDecodeError as e:
                    st.error(f"JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return None
                except Exception as e:
                    st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    import traceback
                    st.text(traceback.format_exc())
                    return None
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨: ä¸¦åˆ—å‡¦ç†ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
            bulk_results = []
            
            try:
                for emb in selected_embeddings:
                    for method, size, overlap in valid_combinations:
                        # ç¾åœ¨ã®è©•ä¾¡ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¡¨ç¤º
                        status_display.info(f"è©•ä¾¡ä¸­: {emb}, {method}, size={size}, overlap={overlap}")
                        
                        # è©•ä¾¡ã‚’å®Ÿè¡Œ
                        result = evaluate_single(emb, method, size, overlap)
                        
                        if result:
                            bulk_results.append(result)
                            status_display.success(f"å®Œäº†: {emb}, {method}, size={size}, overlap={overlap}")
                        else:
                            status_display.warning(f"ã‚¹ã‚­ãƒƒãƒ—: {emb}, {method}, size={size}, overlap={overlap}")
                        
                        # å°‘ã—å¾…æ©Ÿï¼ˆUIã®æ›´æ–°ã®ãŸã‚ï¼‰
                        import time
                        time.sleep(0.1)
                
                # æœ€çµ‚çš„ãªé€²æ—è¡¨ç¤º
                progress_text.success(f"å®Œäº†: {total_tasks} / {total_tasks} ä»¶")
                
                # é€²æ—ãƒãƒ¼ã‚’100%ã«
                progress_bar.progress(1.0)
                
                # æœ€çµ‚çš„ãªã‚µãƒãƒªã‚’è¡¨ç¤º
                if bulk_results:
                    status_display.success(f"è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼ (æˆåŠŸ: {len(bulk_results)}ä»¶)")
                else:
                    status_display.error("è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸãŒã€æœ‰åŠ¹ãªçµæœã¯å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            except Exception as e:
                status_display.error(f"è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            
            # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            if bulk_results:
                try:
                    # çµæœãŒãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã®å ´åˆã«ãƒ•ãƒ©ãƒƒãƒˆåŒ–
                    flat_results = []
                    for result in bulk_results:
                        if isinstance(result, list):
                            flat_results.extend(result)
                        else:
                            flat_results.append(result)
                    
                    st.session_state.bulk_evaluation_results = flat_results
                except Exception as e:
                    status_display.error(f"çµæœã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            else:
                status_display.error("ä¸€æ‹¬è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
                
            # çµæœã®è©³ç´°ã¯ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›
            if bulk_results:
                print("\n=== è©•ä¾¡çµæœã‚µãƒãƒª ===")
                print(f"æˆåŠŸ: {len(bulk_results)}ä»¶")
                for i, result in enumerate(bulk_results, 1):
                    print(f"\n--- çµæœ {i} ---")
                    print(json.dumps(result, ensure_ascii=False, indent=2))

    if st.session_state.bulk_evaluation_results:
        st.subheader("ä¸€æ‹¬è©•ä¾¡çµæœ")
        st.write("ä¸€æ‹¬è©•ä¾¡APIã®è¿”å´å†…å®¹:", st.session_state.bulk_evaluation_results)  # è¿”å´å†…å®¹ã‚’ç¢ºèªç”¨ã«è¡¨ç¤º

        # çµæœã‚’DataFrameã«å¤‰æ›
        eval_results = st.session_state.bulk_evaluation_results
        if isinstance(eval_results, list):
            results_df = pd.DataFrame(eval_results)
        else:
            results_df = pd.DataFrame([eval_results])
        
        # å¿…è¦ã‚«ãƒ©ãƒ è£œå®Œãƒ»ãƒ©ãƒ™ãƒ«åˆ—è¿½åŠ 
        required_cols = {
            'avg_chunk_len', 'num_chunks', 'overall_score', 'chunk_strategy', 'embedding_model',
            'faithfulness', 'answer_relevancy', 'context_recall', 'context_precision', 'answer_correctness'
        }
        missing_cols = required_cols - set(results_df.columns)
        
        if 'chunk_method' in results_df.columns and 'chunk_strategy' not in results_df.columns:
            results_df['chunk_strategy'] = results_df['chunk_method']
            st.info('chunk_strategyåˆ—ã‚’chunk_methodã‹ã‚‰è£œå®Œã—ã¾ã—ãŸ')
        
        if len(results_df) > 0:
            # ä¸è¶³ã‚«ãƒ©ãƒ ã®è£œå®Œ
            if missing_cols:
                st.info(f'ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆç”¨ã®ã‚«ãƒ©ãƒ ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing_cols}ã€‚è‡ªå‹•ã§ä»®å€¤ã‚’è£œå®Œã—ã¾ã™ã€‚')
                for col in missing_cols:
                    if col == 'chunk_strategy':
                        results_df[col] = 'unknown'
                    else:
                        results_df[col] = 0.0
            
            # ãƒ©ãƒ™ãƒ«åˆ—ã‚’è¿½åŠ ï¼ˆchunk_sizeãŒã‚ã‚Œã°å«ã‚ã‚‹ï¼‰
            if 'chunk_size' in results_df.columns:
                results_df['label'] = results_df['chunk_strategy'] + '-' + results_df['chunk_size'].astype(str)
            else:
                results_df['label'] = results_df['chunk_strategy']
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ãã®æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’å®šç¾©
            metrics = ["faithfulness", "answer_relevancy", "context_recall", "context_precision", "answer_correctness"]
            metrics_jp = ["ä¿¡é ¼æ€§", "å›ç­”ã®é–¢é€£æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†ç¾æ€§", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ­£ç¢ºæ€§", "å›ç­”ã®æ­£ç¢ºæ€§"]
            
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if 'embedding_model' in results_df.columns:
                model_groups = list(results_df.groupby('embedding_model'))
            else:
                model_groups = [('default', results_df)]

            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
            for model_name, model_data in model_groups:
                if not model_data.empty and 'chunk_size' in model_data.columns and 'overall_score' in model_data.columns:
                    # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
                    fig_bubble = px.scatter(
                        model_data,
                        x="num_chunks",
                        y="avg_chunk_len",
                        size=[min(s * 20, 50) for s in model_data["overall_score"]],
                        color="overall_score",
                        hover_name=model_data['chunk_strategy'] + '-' + model_data['chunk_size'].astype(str),
                        text=model_data['chunk_strategy'],
                        title=f"{model_name} - ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                        labels={
                            "num_chunks": "ãƒãƒ£ãƒ³ã‚¯æ•°",
                            "avg_chunk_len": "å¹³å‡ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º (æ–‡å­—æ•°)",
                            "overall_score": "ç·åˆã‚¹ã‚³ã‚¢"
                        },
                        color_continuous_scale=px.colors.sequential.Viridis,
                        color_continuous_midpoint=0.5,
                    )
                    
                    # ãƒãƒ–ãƒ«ãƒãƒ£ãƒ¼ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ›´æ–°
                    fig_bubble.update_traces(
                        textposition='middle center',
                        textfont=dict(size=12, color='white', family='Arial'),
                        marker=dict(line=dict(width=1, color='DarkSlateGrey'), opacity=0.8),
                        hovertemplate=
                        '<b>%{hovertext}</b><br>' +
                        'ãƒãƒ£ãƒ³ã‚¯æ•°: %{x}<br>' +
                        'å¹³å‡ã‚µã‚¤ã‚º: %{y}æ–‡å­—<br>' +
                        'ã‚¹ã‚³ã‚¢: %{marker.color:.2f}<extra></extra>',
                    )
                    
                    fig_bubble.update_layout(
                        title={
                            'text': f"{model_name} - ãƒãƒ£ãƒ³ã‚¯åˆ†å¸ƒã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                            'x': 0.5,
                            'xanchor': 'center'
                        },
                        coloraxis_colorbar=dict(title="ã‚¹ã‚³ã‚¢"),
                        font=dict(size=14),
                        height=500,
                        margin=dict(l=40, r=40, t=80, b=40)
                    )
                    
                    st.plotly_chart(fig_bubble, use_container_width=True)
                    st.markdown('<br>', unsafe_allow_html=True)
            
            # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
            for model_name, model_data in model_groups:
                if not model_data.empty and 'chunk_strategy' in model_data.columns and 'overall_score' in model_data.columns:
                    # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã”ã¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é›†è¨ˆ
                    strategy_scores = model_data.groupby('chunk_strategy')['overall_score'].mean().sort_values(ascending=False)
                    
                    # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ
                    fig_bar = px.bar(
                        x=strategy_scores.values,
                        y=strategy_scores.index,
                        orientation='h',
                        title=f"{model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                        labels={'x': 'å¹³å‡ã‚¹ã‚³ã‚¢', 'y': 'ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥'},
                        color=strategy_scores.values,
                        color_continuous_scale=px.colors.sequential.Viridis,
                    )
                    
                    # ãƒãƒ¼ã®ä¸Šã«ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤º
                    fig_bar.update_traces(
                        texttemplate='%{x:.3f}',
                        textposition='outside',
                        hovertemplate='<b>%{y}</b><br>ã‚¹ã‚³ã‚¢: %{x:.3f}<extra></extra>',
                    )
                    
                    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                    fig_bar.update_layout(
                        title={
                            'text': f"{model_name} - ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
                            'x': 0.5,
                            'xanchor': 'center',
                            'font': {'size': 18}
                        },
                        xaxis=dict(range=[0, 1.1]),
                        coloraxis_showscale=False,
                        height=400,
                        margin=dict(l=100, r=40, t=100, b=40),
                        yaxis=dict(autorange="reversed"),
                        font=dict(size=14)
                    )
                    
                    # ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
                    st.plotly_chart(fig_bar, use_container_width=True)
                    st.markdown('<br>', unsafe_allow_html=True)
            
            # ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã”ã¨ã«ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã‚’è¡¨ç¤º
            if 'chunk_strategy' in results_df.columns:
                chunk_strategies = results_df['chunk_strategy'].unique()
                
                for strategy in chunk_strategies:
                    strategy_data = results_df[results_df['chunk_strategy'] == strategy]
                    
                    if not strategy_data.empty:
                        st.subheader(f"{strategy} - è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒ")
                        fig_radar = go.Figure()
                        
                        # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                        for model_name, model_data in model_groups:
                            model_strategy_data = strategy_data[strategy_data['embedding_model'] == model_name] if 'embedding_model' in strategy_data.columns else strategy_data
                            
                            if not model_strategy_data.empty:
                                # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¹³å‡å€¤ã‚’è¨ˆç®—
                                r_values = [model_strategy_data[m].mean() if m in model_strategy_data.columns else 0.5 for m in metrics]
                                
                                fig_radar.add_trace(go.Scatterpolar(
                                    r=r_values,
                                    theta=metrics_jp,
                                    fill='toself',
                                    name=model_name,
                                    hovertemplate='%{theta}: %{r:.2f}<extra></extra>',
                                    line=dict(width=2)
                                ))
                        
                        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®èª¿æ•´
                        fig_radar.update_layout(
                            polar=dict(
                                radialaxis=dict(
                                    visible=True,
                                    range=[0, 1],
                                    tickfont=dict(size=10),
                                    tickangle=0,
                                    tickformat='.1f',
                                    gridwidth=1
                                ),
                                angularaxis=dict(
                                    rotation=90,
                                    direction='clockwise',
                                    tickfont=dict(size=12),
                                    gridwidth=1
                                ),
                                bgcolor='rgba(0,0,0,0.02)'
                            ),
                            showlegend=True,
                            legend=dict(
                                orientation='h',
                                yanchor='bottom',
                                y=1.15,
                                xanchor='center',
                                x=0.5,
                                font=dict(size=12)
                            ),
                            margin=dict(l=60, r=60, t=30, b=60),  # ä¸Šéƒ¨ãƒãƒ¼ã‚¸ãƒ³ã‚’å°ã•ãèª¿æ•´
                            height=500,
                            paper_bgcolor='rgba(0,0,0,0)',
                            plot_bgcolor='rgba(0,0,0,0)'
                        )
                        
                        st.plotly_chart(fig_radar, use_container_width=True)
                        st.markdown('<br>', unsafe_allow_html=True)
            
            # çµæœã‚’DataFrameã«å¤‰æ›
        results_df = pd.DataFrame(st.session_state.bulk_evaluation_results)

with tab4:
    if 'uploaded_file_bytes' not in st.session_state:
        st.warning("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
        
    st.header("è©•ä¾¡çµæœã®æ¯”è¼ƒ")
    
    if 'evaluation_results' in st.session_state and st.session_state.evaluation_results:
        st.subheader("è©•ä¾¡çµæœã®æ¯”è¼ƒ")
        
        # è©•ä¾¡çµæœã‚’DataFrameã«å¤‰æ›
        eval_results = st.session_state.evaluation_results
        if 'results' in eval_results:
            df_data = []
            for i, result in enumerate(eval_results['results']):
                row = {
                    'è³ªå•ç•ªå·': i+1,
                    'è³ªå•': result.get('question', ''),
                    'å›ç­”': result.get('answer', '')
                }
                if 'details' in result:
                    for k, v in result['details'].items():
                        if isinstance(v, (int, float)):
                            row[k] = v
                df_data.append(row)
            
            if df_data:
                df = pd.DataFrame(df_data)
                st.dataframe(df)
                
                # ã‚¹ã‚³ã‚¢ã®å¯è¦–åŒ–
                score_cols = [col for col in df.columns if col not in ['è³ªå•ç•ªå·', 'è³ªå•', 'å›ç­”']]
                if score_cols:
                    st.subheader("ã‚¹ã‚³ã‚¢ã®æ¯”è¼ƒ")
                    fig = px.bar(df, x='è³ªå•ç•ªå·', y=score_cols, 
                                title="è³ªå•ã”ã¨ã®ã‚¹ã‚³ã‚¢æ¯”è¼ƒ",
                                labels={'value': 'ã‚¹ã‚³ã‚¢', 'variable': 'è©•ä¾¡é …ç›®', 'è³ªå•ç•ªå·': 'è³ªå•ç•ªå·'})
                    st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("æ¯”è¼ƒã™ã‚‹è©•ä¾¡çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚è©•ä¾¡ã¾ãŸã¯ä¸€æ‹¬è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        # --- è‡ªå‹•ç”ŸæˆQAã‚»ãƒƒãƒˆå„ªå…ˆã§åˆ©ç”¨ ---
        auto_questions = st.session_state.get('qa_questions', None)
        auto_answers = st.session_state.get('qa_answers', None)
        if auto_questions:
            st.markdown("#### ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰PDFã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸè³ªå•ã‚»ãƒƒãƒˆã‚’åˆ©ç”¨ã—ã¾ã™")
            questions = auto_questions
            st.write("\n".join([f"Q{i+1}: {q}" for i, q in enumerate(questions)]))
        else:
            questions_input = st.text_area("1è¡Œã«1ã¤ã®è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=150, 
                                           value="ä¸»ãªãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°æŠ€è¡“ã«ã¯ä½•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\nã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ã¨å†å¸°çš„æ–‡å­—åˆ†å‰²ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ")
            questions = [q.strip() for q in questions_input.split('\n') if q.strip()]

        if st.button("è©•ä¾¡ã‚’å®Ÿè¡Œ", key="evaluate_button_evaluation"):
            if not questions:
                st.warning("æœ€ä½1ã¤ã®è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            else:
                with st.spinner("è©•ä¾¡ã‚’å®Ÿè¡Œä¸­... ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚"):
                    try:
                        # 1. è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                        answers = []
                        contexts = []
                        # --- ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰PDFã‹ã‚‰è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚»ãƒƒãƒˆãŒã‚ã‚Œã°åˆ©ç”¨ ---
                        if auto_answers and len(auto_answers) == len(questions):
                            answers = auto_answers
                            st.success("è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                        else:
                            # å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã«è¿½åŠ 
                            st.warning("è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå›ç­”ã‚»ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                            st.stop()

                        # 2. è©•ä¾¡ã‚’å®Ÿè¡Œ
                        evaluation_payload = {
                            "questions": questions,
                            "answers": answers,
                            "contexts": ["" for _ in questions],  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ç©ºã§ä»®è¨­å®š
                            "model": st.session_state.llm_model
                        }
                        
                        eval_response = requests.post(f"{BACKEND_URL}/evaluate/", json=evaluation_payload)
                        if eval_response.status_code == 200:
                            st.session_state.evaluation_results = eval_response.json()
                            st.success("è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                            # è©•ä¾¡çµæœã‚’è¡¨ç¤ºã™ã‚‹ã‚¿ãƒ–ã«ç§»å‹•
                            st.session_state.active_tab = "è©•ä¾¡"
                            st.rerun()
                        else:
                            st.error(f"è©•ä¾¡ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {eval_response.text}")
                    except Exception as e:
                        st.error(f"è©•ä¾¡ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    st.warning(f"chunk_size <= overlap ã¨ãªã‚‹ä¸æ­£ãªçµ„ã¿åˆã‚ã›ã¯è‡ªå‹•çš„ã«é™¤å¤–ã—ã¾ã—ãŸ: ({embedding}, {strategy}, {size}, {overlap})")
                    # é€²æ—ãƒãƒ¼ã®è¨­å®š
                    progress_bar = st.progress(0)
                    total_tasks = len(futures)
                    completed_tasks = 0
                    for future in concurrent.futures.as_completed(futures):
                        result = future.result()
                        if result is not None:
                            answers.append(result['answer'])
                            contexts.append(result['context'])
                        completed_tasks += 1
                        progress_bar.progress(min(completed_tasks / total_tasks, 1.0))
                    progress_bar.empty()
                    st.session_state.evaluation_results = {"answers": answers, "contexts": contexts}
                    st.success("è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

        if st.session_state.evaluation_results:
            st.subheader("è©•ä¾¡æŒ‡æ¨™")
            st.write("è©•ä¾¡APIã®è¿”å´å†…å®¹:", st.session_state.evaluation_results)  # è¿”å´å†…å®¹ã‚’ç¢ºèªç”¨ã«è¡¨ç¤º
            # evaluation_resultsãŒãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã«å¯¾å¿œ
            eval_results = st.session_state.evaluation_results
            if isinstance(eval_results, list):
                results_df = pd.DataFrame(eval_results)
            else:
                results_df = pd.DataFrame([eval_results])
            # è‹±èªâ†’æ—¥æœ¬èªã®æŒ‡æ¨™åãƒãƒƒãƒ”ãƒ³ã‚°
            METRIC_JA = {
                "faithfulness": "ãƒ•ã‚¡ã‚¯ãƒˆæ•´åˆæ€§",
                "answer_relevancy": "å›ç­”é–¢é€£æ€§",
                "context_recall": "æ–‡è„ˆå†ç¾ç‡",
                "context_precision": "æ–‡è„ˆé©åˆç‡"
            }

            # DataFrameã®ã‚«ãƒ©ãƒ åã‚‚æ—¥æœ¬èªã«å¤‰æ›ã—ã¦è¡¨ç¤º
            results_df_ja = results_df.rename(columns=METRIC_JA)
            st.dataframe(results_df_ja)

            # Plotting
            metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
            available_metrics = [m for m in metrics if m in results_df.columns]

            if available_metrics:
                plot_df = results_df[available_metrics].T.reset_index()
                plot_df['index'] = plot_df['index'].map(METRIC_JA)
                plot_df.columns = ['æŒ‡æ¨™', 'ã‚¹ã‚³ã‚¢']

                # Performance Rankingé¢¨ æ¨ªæ£’ã‚°ãƒ©ãƒ•
                fig_bar = px.bar(
                    plot_df,
                    y='æŒ‡æ¨™',
                    x='ã‚¹ã‚³ã‚¢',
                    orientation='h',
                    text='ã‚¹ã‚³ã‚¢',
                    title="Performance Ranking",
                    labels={"æŒ‡æ¨™": "Metric", "ã‚¹ã‚³ã‚¢": "Score"},
                )
                fig_bar.update_traces(texttemplate='%{x:.3f}', textposition='outside')
                fig_bar.update_layout(
                    title={
                        'text': "RAG Strategy Performance Ranking<br><sup>Error bars show standard deviation â€¢ * indicates statistical significance</sup>",
                        'x':0.5,
                        'xanchor': 'center'
                    },
                    font=dict(size=16),
                    height=550,
                    margin=dict(l=40, r=40, t=80, b=40),
                    showlegend=False
                )
                st.plotly_chart(fig_bar, use_container_width=True)
                if 'eval_figs' not in st.session_state:
                    st.session_state['eval_figs'] = []
                if len(st.session_state['eval_figs']) == 0 or st.session_state['eval_figs'][-1] != fig_bar:
                    st.session_state['eval_figs'].append(fig_bar)
                st.markdown('<br>', unsafe_allow_html=True)

                # RAGAS Metrics Comparisoné¢¨ ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
                import plotly.graph_objects as go
                metrics_en = ['Faithfulness', 'Answer Relevancy', 'Context Recall', 'Context Precision']
                metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']
                r_values = [results_df[m].iloc[0] if m in results_df.columns else 0 for m in metrics]
                fig_radar = go.Figure()
                fig_radar.add_trace(go.Scatterpolar(
                    r=r_values,
                    theta=metrics_en,
                    fill='toself',
                    name='Sample'
                ))
                fig_radar.update_layout(
                    title={
                        'text': "RAGAS Metrics Comparison<br><sup>All metrics normalized to 0-1 scale</sup>",
                        'x':0.5,
                        'xanchor': 'center'
                    },
                    polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                    showlegend=True,
                    font=dict(size=16),
                    height=600,
                    margin=dict(l=40, r=40, t=80, b=40)
                )
                st.plotly_chart(fig_radar, use_container_width=True)
                if 'eval_figs' not in st.session_state:
                    st.session_state['eval_figs'] = []
                if len(st.session_state['eval_figs']) == 0 or st.session_state['eval_figs'][-1] != fig_radar:
                    st.session_state['eval_figs'].append(fig_radar)
                st.markdown('<br>', unsafe_allow_html=True)
            else:
                st.warning("è©•ä¾¡æŒ‡æ¨™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚è©•ä¾¡APIã®è¿”å´å†…å®¹ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")

# ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¿ãƒ–
with tab_chatbot:
    st.header("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "chat_messages" not in st.session_state:
        st.session_state.chat_messages = []
    
    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    for message in st.session_state.chat_messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        st.session_state.chat_messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§å¿œç­”ã‚’ç”Ÿæˆ
        response_text = ""
        with st.chat_message("assistant"):
            try:
                if not os.getenv("OPENAI_API_KEY"):
                    st.error("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã«OPENAI_API_KEYã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                    response_text = "APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
                messages = [{"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"}]
                messages.extend([{"role": m["role"], "content": m["content"]} for m in st.session_state.chat_messages])
                
                # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦APIã‚’å‘¼ã³å‡ºã—
                if st.session_state.chat_model in ["gpt-4o-mini", "gpt-3.5-turbo"]:
                    # OpenAI APIã‚’ä½¿ç”¨
                    api_key = os.getenv("OPENAI_API_KEY")
                    if not api_key:
                        response_text = "ã‚¨ãƒ©ãƒ¼: APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                    else:
                        try:
                            client = OpenAI(api_key=api_key)
                            response = client.chat.completions.create(
                                model=st.session_state.chat_model,
                                messages=messages,
                                temperature=0.7,
                                max_tokens=1000
                            )
                            response_text = response.choices[0].message.content
                        except Exception as e:
                            response_text = f"APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                else:
                    # ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼ˆä¾‹ã¨ã—ã¦ã®å®Ÿè£…ï¼‰
                    response_text = f"{st.session_state.chat_model} ã‹ã‚‰ã®å¿œç­”: ã‚ãªãŸã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€Œ{prompt}ã€ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚\n\nï¼ˆæ³¨: ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ãƒ€ãƒŸãƒ¼å¿œç­”ã§ã™ï¼‰"
                
                st.markdown(response_text)
                
                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ 
                st.session_state.chat_messages.append({"role": "assistant", "content": response_text})
                
            except Exception as e:
                error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                st.error(error_msg)
                st.session_state.chat_messages.append({"role": "assistant", "content": error_msg})
        
        # ç”»é¢ã‚’æ›´æ–°
        st.rerun()